{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AllenNLP Exploitation Ambiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0729 22:14:41.159642 4508097984 file_utils.py:41] PyTorch version 1.5.0 available.\n",
      "I0729 22:14:45.267963 4508097984 archival.py:164] loading archive file https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz from cache at /Users/sakshiudeshi/.allennlp/cache/0f6b052811b20b13280e609a96efe71ebc636b9c823a5c906ba24459e6e68af9.c1dab61d84cc7c3f7d6751c260040607cb7023a002778ba8f9b9d196b6539174\n",
      "I0729 22:14:45.269434 4508097984 archival.py:171] extracting archive file /Users/sakshiudeshi/.allennlp/cache/0f6b052811b20b13280e609a96efe71ebc636b9c823a5c906ba24459e6e68af9.c1dab61d84cc7c3f7d6751c260040607cb7023a002778ba8f9b9d196b6539174 to temp dir /var/folders/fj/wgtgbbdj0h15ng8x_hqcdw940000gn/T/tmp3w7tly43\n",
      "I0729 22:14:53.435321 4508097984 params.py:247] type = from_instances\n",
      "I0729 22:14:53.436029 4508097984 vocabulary.py:314] Loading token dictionary from /var/folders/fj/wgtgbbdj0h15ng8x_hqcdw940000gn/T/tmp3w7tly43/vocabulary.\n",
      "I0729 22:14:53.438181 4508097984 params.py:247] model.type = coref\n",
      "I0729 22:14:53.439244 4508097984 params.py:247] model.regularizer = None\n",
      "I0729 22:14:53.440451 4508097984 params.py:247] model.text_field_embedder.type = basic\n",
      "I0729 22:14:53.441848 4508097984 params.py:247] model.text_field_embedder.token_embedders.tokens.type = pretrained_transformer_mismatched\n",
      "I0729 22:14:53.442851 4508097984 params.py:247] model.text_field_embedder.token_embedders.tokens.model_name = SpanBERT/spanbert-large-cased\n",
      "I0729 22:14:53.443603 4508097984 params.py:247] model.text_field_embedder.token_embedders.tokens.max_length = 512\n",
      "I0729 22:14:54.693216 4508097984 configuration_utils.py:283] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/config.json from cache at /Users/sakshiudeshi/.cache/torch/transformers/e4945c09a81a5208c47d4c3fedcb96eaa270b74d3702499570750e460ef48607.117d86ec2a76573fb7f175740de3f663d3a7206b50fcbeeb662cb1f68e329786\n",
      "I0729 22:14:54.694777 4508097984 configuration_utils.py:319] Model config BertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": null,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0729 22:14:55.817948 4508097984 modeling_utils.py:507] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/pytorch_model.bin from cache at /Users/sakshiudeshi/.cache/torch/transformers/d707dadfcbbac6a5fc440f1e94db728b000a2816693f44a87092182199f2d52d.d1ce6dff7f84348ad7c77a33a9a6e8751099db9c9d609ac7752e61804befe4da\n",
      "I0729 22:14:59.350640 4508097984 modeling_utils.py:601] Weights of BertModel not initialized from pretrained model: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "I0729 22:15:00.595384 4508097984 configuration_utils.py:283] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/config.json from cache at /Users/sakshiudeshi/.cache/torch/transformers/e4945c09a81a5208c47d4c3fedcb96eaa270b74d3702499570750e460ef48607.117d86ec2a76573fb7f175740de3f663d3a7206b50fcbeeb662cb1f68e329786\n",
      "I0729 22:15:00.596815 4508097984 configuration_utils.py:319] Model config BertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": null,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0729 22:15:00.597735 4508097984 tokenization_utils.py:420] Model name 'SpanBERT/spanbert-large-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'SpanBERT/spanbert-large-cased' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0729 22:15:06.337690 4508097984 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/vocab.txt from cache at /Users/sakshiudeshi/.cache/torch/transformers/3c791b35663b47a1c79ed04d06cd628f11a0e1ac5248c736e3e63437dd140820.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "I0729 22:15:06.338567 4508097984 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/added_tokens.json from cache at None\n",
      "I0729 22:15:06.339246 4508097984 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/special_tokens_map.json from cache at None\n",
      "I0729 22:15:06.339987 4508097984 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/tokenizer_config.json from cache at None\n",
      "I0729 22:15:06.376595 4508097984 params.py:247] model.context_layer.type = pass_through\n",
      "I0729 22:15:06.377436 4508097984 params.py:247] model.context_layer.input_dim = 1024\n",
      "I0729 22:15:06.378050 4508097984 params.py:247] model.mention_feedforward.input_dim = 3092\n",
      "I0729 22:15:06.378669 4508097984 params.py:247] model.mention_feedforward.num_layers = 2\n",
      "I0729 22:15:06.379200 4508097984 params.py:247] model.mention_feedforward.hidden_dims = 1500\n",
      "I0729 22:15:06.380058 4508097984 params.py:247] model.mention_feedforward.activations = relu\n",
      "I0729 22:15:06.380635 4508097984 params.py:247] type = relu\n",
      "I0729 22:15:06.381661 4508097984 params.py:247] model.mention_feedforward.dropout = 0.3\n",
      "I0729 22:15:06.420325 4508097984 params.py:247] model.antecedent_feedforward.input_dim = 9296\n",
      "I0729 22:15:06.421159 4508097984 params.py:247] model.antecedent_feedforward.num_layers = 2\n",
      "I0729 22:15:06.421619 4508097984 params.py:247] model.antecedent_feedforward.hidden_dims = 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0729 22:15:06.422438 4508097984 params.py:247] model.antecedent_feedforward.activations = relu\n",
      "I0729 22:15:06.423303 4508097984 params.py:247] type = relu\n",
      "I0729 22:15:06.424170 4508097984 params.py:247] model.antecedent_feedforward.dropout = 0.3\n",
      "I0729 22:15:06.509786 4508097984 params.py:247] model.feature_size = 20\n",
      "I0729 22:15:06.510450 4508097984 params.py:247] model.max_span_width = 30\n",
      "I0729 22:15:06.510921 4508097984 params.py:247] model.spans_per_word = 0.4\n",
      "I0729 22:15:06.511628 4508097984 params.py:247] model.max_antecedents = 50\n",
      "I0729 22:15:06.512071 4508097984 params.py:247] model.coarse_to_fine = True\n",
      "I0729 22:15:06.513090 4508097984 params.py:247] model.inference_order = 2\n",
      "I0729 22:15:06.513778 4508097984 params.py:247] model.lexical_dropout = 0.2\n",
      "I0729 22:15:06.514824 4508097984 params.py:247] model.initializer.regexes.0.1.type = xavier_normal\n",
      "I0729 22:15:06.515836 4508097984 params.py:247] model.initializer.regexes.0.1.gain = 1.0\n",
      "I0729 22:15:06.516983 4508097984 params.py:247] model.initializer.regexes.1.1.type = xavier_normal\n",
      "I0729 22:15:06.517664 4508097984 params.py:247] model.initializer.regexes.1.1.gain = 1.0\n",
      "I0729 22:15:06.518607 4508097984 params.py:247] model.initializer.regexes.2.1.type = xavier_normal\n",
      "I0729 22:15:06.519309 4508097984 params.py:247] model.initializer.regexes.2.1.gain = 1.0\n",
      "I0729 22:15:06.520122 4508097984 params.py:247] model.initializer.regexes.3.1.type = xavier_normal\n",
      "I0729 22:15:06.520884 4508097984 params.py:247] model.initializer.regexes.3.1.gain = 1.0\n",
      "I0729 22:15:06.521739 4508097984 params.py:247] model.initializer.regexes.4.1.type = xavier_normal\n",
      "I0729 22:15:06.522530 4508097984 params.py:247] model.initializer.regexes.4.1.gain = 1.0\n",
      "I0729 22:15:06.523255 4508097984 params.py:247] model.initializer.regexes.5.1.type = xavier_normal\n",
      "I0729 22:15:06.523952 4508097984 params.py:247] model.initializer.regexes.5.1.gain = 1.0\n",
      "I0729 22:15:06.524749 4508097984 params.py:247] model.initializer.regexes.6.1.type = orthogonal\n",
      "I0729 22:15:06.525297 4508097984 params.py:247] model.initializer.regexes.6.1.gain = 1.0\n",
      "I0729 22:15:06.526128 4508097984 params.py:247] model.initializer.prevent_regexes = None\n",
      "I0729 22:15:06.582717 4508097984 initializers.py:471] Initializing parameters\n",
      "I0729 22:15:06.604901 4508097984 initializers.py:481] Initializing _mention_feedforward._module._linear_layers.0.weight using .*linear_layers.*weight initializer\n",
      "I0729 22:15:06.625597 4508097984 initializers.py:481] Initializing _mention_feedforward._module._linear_layers.1.weight using .*linear_layers.*weight initializer\n",
      "I0729 22:15:06.636923 4508097984 initializers.py:481] Initializing _mention_scorer._module.weight using .*scorer.*weight initializer\n",
      "I0729 22:15:06.637712 4508097984 initializers.py:481] Initializing _antecedent_feedforward._module._linear_layers.0.weight using .*linear_layers.*weight initializer\n",
      "I0729 22:15:06.703155 4508097984 initializers.py:481] Initializing _antecedent_feedforward._module._linear_layers.1.weight using .*linear_layers.*weight initializer\n",
      "I0729 22:15:06.714748 4508097984 initializers.py:481] Initializing _antecedent_scorer._module.weight using .*scorer.*weight initializer\n",
      "I0729 22:15:06.715508 4508097984 initializers.py:481] Initializing _endpoint_span_extractor._span_width_embedding.weight using _span_width_embedding.weight initializer\n",
      "I0729 22:15:06.716205 4508097984 initializers.py:481] Initializing _distance_embedding.weight using _distance_embedding.weight initializer\n",
      "I0729 22:15:06.717140 4508097984 initializers.py:481] Initializing _coarse2fine_scorer.weight using .*scorer.*weight initializer\n",
      "I0729 22:15:06.762867 4508097984 initializers.py:481] Initializing _span_updating_gated_sum._gate.weight using .*_span_updating_gated_sum.*weight initializer\n",
      "W0729 22:15:06.763731 4508097984 initializers.py:488] Did not use initialization regex that was passed: _context_layer._module.weight_hh.*\n",
      "W0729 22:15:06.764302 4508097984 initializers.py:488] Did not use initialization regex that was passed: _context_layer._module.weight_ih.*\n",
      "I0729 22:15:06.764922 4508097984 initializers.py:490] Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "I0729 22:15:06.765537 4508097984 initializers.py:496]    _antecedent_feedforward._module._linear_layers.0.bias\n",
      "I0729 22:15:06.766289 4508097984 initializers.py:496]    _antecedent_feedforward._module._linear_layers.1.bias\n",
      "I0729 22:15:06.766839 4508097984 initializers.py:496]    _antecedent_scorer._module.bias\n",
      "I0729 22:15:06.767674 4508097984 initializers.py:496]    _attentive_span_extractor._global_attention._module.bias\n",
      "I0729 22:15:06.768229 4508097984 initializers.py:496]    _attentive_span_extractor._global_attention._module.weight\n",
      "I0729 22:15:06.769007 4508097984 initializers.py:496]    _coarse2fine_scorer.bias\n",
      "I0729 22:15:06.769457 4508097984 initializers.py:496]    _mention_feedforward._module._linear_layers.0.bias\n",
      "I0729 22:15:06.770240 4508097984 initializers.py:496]    _mention_feedforward._module._linear_layers.1.bias\n",
      "I0729 22:15:06.770951 4508097984 initializers.py:496]    _mention_scorer._module.bias\n",
      "I0729 22:15:06.771646 4508097984 initializers.py:496]    _span_updating_gated_sum._gate.bias\n",
      "I0729 22:15:06.772283 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.LayerNorm.bias\n",
      "I0729 22:15:06.772880 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.LayerNorm.weight\n",
      "I0729 22:15:06.773350 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.position_embeddings.weight\n",
      "I0729 22:15:06.774135 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.token_type_embeddings.weight\n",
      "I0729 22:15:06.774615 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.word_embeddings.weight\n",
      "I0729 22:15:06.775337 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "I0729 22:15:06.775882 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "I0729 22:15:06.776563 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.bias\n",
      "I0729 22:15:06.777148 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.weight\n",
      "I0729 22:15:06.777906 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.bias\n",
      "I0729 22:15:06.778394 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.weight\n",
      "I0729 22:15:06.779041 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.bias\n",
      "I0729 22:15:06.779599 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.weight\n",
      "I0729 22:15:06.780375 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.bias\n",
      "I0729 22:15:06.781013 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.weight\n",
      "I0729 22:15:06.781734 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.bias\n",
      "I0729 22:15:06.782116 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0729 22:15:06.782746 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.bias\n",
      "I0729 22:15:06.783307 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.weight\n",
      "I0729 22:15:06.783791 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.dense.bias\n",
      "I0729 22:15:06.784255 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.dense.weight\n",
      "I0729 22:15:06.784841 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "I0729 22:15:06.785438 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "I0729 22:15:06.785900 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.bias\n",
      "I0729 22:15:06.786359 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.weight\n",
      "I0729 22:15:06.786856 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.bias\n",
      "I0729 22:15:06.787407 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.weight\n",
      "I0729 22:15:06.787858 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.bias\n",
      "I0729 22:15:06.788491 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.weight\n",
      "I0729 22:15:06.789016 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.bias\n",
      "I0729 22:15:06.789840 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.weight\n",
      "I0729 22:15:06.790361 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.bias\n",
      "I0729 22:15:06.790925 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.weight\n",
      "I0729 22:15:06.791395 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.bias\n",
      "I0729 22:15:06.791911 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.weight\n",
      "I0729 22:15:06.792433 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.dense.bias\n",
      "I0729 22:15:06.792932 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.dense.weight\n",
      "I0729 22:15:06.793528 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "I0729 22:15:06.794194 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "I0729 22:15:06.794672 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.bias\n",
      "I0729 22:15:06.795355 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.weight\n",
      "I0729 22:15:06.795787 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.bias\n",
      "I0729 22:15:06.796424 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.weight\n",
      "I0729 22:15:06.797002 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.bias\n",
      "I0729 22:15:06.797797 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.weight\n",
      "I0729 22:15:06.798354 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.bias\n",
      "I0729 22:15:06.799113 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.weight\n",
      "I0729 22:15:06.799509 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.bias\n",
      "I0729 22:15:06.800104 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.weight\n",
      "I0729 22:15:06.800612 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.bias\n",
      "I0729 22:15:06.801280 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.weight\n",
      "I0729 22:15:06.801769 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.dense.bias\n",
      "I0729 22:15:06.802512 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.dense.weight\n",
      "I0729 22:15:06.803020 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "I0729 22:15:06.803822 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "I0729 22:15:06.804432 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.bias\n",
      "I0729 22:15:06.804913 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.weight\n",
      "I0729 22:15:06.805473 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.bias\n",
      "I0729 22:15:06.806346 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.weight\n",
      "I0729 22:15:06.806799 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.bias\n",
      "I0729 22:15:06.807368 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.weight\n",
      "I0729 22:15:06.807963 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.bias\n",
      "I0729 22:15:06.808588 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0729 22:15:06.809053 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.bias\n",
      "I0729 22:15:06.809852 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.weight\n",
      "I0729 22:15:06.810396 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.bias\n",
      "I0729 22:15:06.811095 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.weight\n",
      "I0729 22:15:06.811726 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.dense.bias\n",
      "I0729 22:15:06.812646 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.dense.weight\n",
      "I0729 22:15:06.813157 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.LayerNorm.bias\n",
      "I0729 22:15:06.813860 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.LayerNorm.weight\n",
      "I0729 22:15:06.814419 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.dense.bias\n",
      "I0729 22:15:06.815034 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.dense.weight\n",
      "I0729 22:15:06.815679 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.key.bias\n",
      "I0729 22:15:06.816314 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.key.weight\n",
      "I0729 22:15:06.816864 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.query.bias\n",
      "I0729 22:15:06.817732 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.query.weight\n",
      "I0729 22:15:06.818228 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.value.bias\n",
      "I0729 22:15:06.818866 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.value.weight\n",
      "I0729 22:15:06.819419 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.intermediate.dense.bias\n",
      "I0729 22:15:06.819925 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.intermediate.dense.weight\n",
      "I0729 22:15:06.820470 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.LayerNorm.bias\n",
      "I0729 22:15:06.821008 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.LayerNorm.weight\n",
      "I0729 22:15:06.821501 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.dense.bias\n",
      "I0729 22:15:06.821893 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.dense.weight\n",
      "I0729 22:15:06.822563 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.LayerNorm.bias\n",
      "I0729 22:15:06.823213 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.LayerNorm.weight\n",
      "I0729 22:15:06.823710 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.dense.bias\n",
      "I0729 22:15:06.824182 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.dense.weight\n",
      "I0729 22:15:06.824664 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.key.bias\n",
      "I0729 22:15:06.825181 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.key.weight\n",
      "I0729 22:15:06.825594 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.query.bias\n",
      "I0729 22:15:06.825999 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.query.weight\n",
      "I0729 22:15:06.826586 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.value.bias\n",
      "I0729 22:15:06.827049 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.value.weight\n",
      "I0729 22:15:06.827619 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.intermediate.dense.bias\n",
      "I0729 22:15:06.828261 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.intermediate.dense.weight\n",
      "I0729 22:15:06.828715 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.LayerNorm.bias\n",
      "I0729 22:15:06.829289 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.LayerNorm.weight\n",
      "I0729 22:15:06.829872 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.dense.bias\n",
      "I0729 22:15:06.830395 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.dense.weight\n",
      "I0729 22:15:06.830976 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.LayerNorm.bias\n",
      "I0729 22:15:06.831656 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.LayerNorm.weight\n",
      "I0729 22:15:06.832082 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.dense.bias\n",
      "I0729 22:15:06.832730 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.dense.weight\n",
      "I0729 22:15:06.833290 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.key.bias\n",
      "I0729 22:15:06.833850 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.key.weight\n",
      "I0729 22:15:06.834285 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.query.bias\n",
      "I0729 22:15:06.834818 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.query.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0729 22:15:06.835420 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.value.bias\n",
      "I0729 22:15:06.835911 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.value.weight\n",
      "I0729 22:15:06.836730 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.intermediate.dense.bias\n",
      "I0729 22:15:06.837279 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.intermediate.dense.weight\n",
      "I0729 22:15:06.838033 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.LayerNorm.bias\n",
      "I0729 22:15:06.838725 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.LayerNorm.weight\n",
      "I0729 22:15:06.839502 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.dense.bias\n",
      "I0729 22:15:06.840178 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.dense.weight\n",
      "I0729 22:15:06.840747 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.LayerNorm.bias\n",
      "I0729 22:15:06.841202 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.LayerNorm.weight\n",
      "I0729 22:15:06.841831 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.dense.bias\n",
      "I0729 22:15:06.842390 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.dense.weight\n",
      "I0729 22:15:06.842981 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.key.bias\n",
      "I0729 22:15:06.843810 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.key.weight\n",
      "I0729 22:15:06.844464 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.query.bias\n",
      "I0729 22:15:06.845296 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.query.weight\n",
      "I0729 22:15:06.845973 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.value.bias\n",
      "I0729 22:15:06.846806 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.value.weight\n",
      "I0729 22:15:06.847387 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.intermediate.dense.bias\n",
      "I0729 22:15:06.847951 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.intermediate.dense.weight\n",
      "I0729 22:15:06.848431 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.LayerNorm.bias\n",
      "I0729 22:15:06.849049 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.LayerNorm.weight\n",
      "I0729 22:15:06.849457 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.dense.bias\n",
      "I0729 22:15:06.849889 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.dense.weight\n",
      "I0729 22:15:06.850693 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.LayerNorm.bias\n",
      "I0729 22:15:06.851277 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.LayerNorm.weight\n",
      "I0729 22:15:06.851871 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.dense.bias\n",
      "I0729 22:15:06.852442 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.dense.weight\n",
      "I0729 22:15:06.853026 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.key.bias\n",
      "I0729 22:15:06.853552 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.key.weight\n",
      "I0729 22:15:06.854264 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.query.bias\n",
      "I0729 22:15:06.854769 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.query.weight\n",
      "I0729 22:15:06.855365 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.value.bias\n",
      "I0729 22:15:06.856095 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.value.weight\n",
      "I0729 22:15:06.856687 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.intermediate.dense.bias\n",
      "I0729 22:15:06.857244 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.intermediate.dense.weight\n",
      "I0729 22:15:06.857861 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.LayerNorm.bias\n",
      "I0729 22:15:06.858277 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.LayerNorm.weight\n",
      "I0729 22:15:06.858686 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.dense.bias\n",
      "I0729 22:15:06.859503 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.dense.weight\n",
      "I0729 22:15:06.860079 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.LayerNorm.bias\n",
      "I0729 22:15:06.860855 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.LayerNorm.weight\n",
      "I0729 22:15:06.861405 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.dense.bias\n",
      "I0729 22:15:06.861928 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.dense.weight\n",
      "I0729 22:15:06.862448 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.key.bias\n",
      "I0729 22:15:06.863077 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.key.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0729 22:15:06.863569 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.query.bias\n",
      "I0729 22:15:06.864091 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.query.weight\n",
      "I0729 22:15:06.864621 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.value.bias\n",
      "I0729 22:15:06.865154 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.value.weight\n",
      "I0729 22:15:06.865586 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.intermediate.dense.bias\n",
      "I0729 22:15:06.866142 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.intermediate.dense.weight\n",
      "I0729 22:15:06.866649 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.LayerNorm.bias\n",
      "I0729 22:15:06.867060 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.LayerNorm.weight\n",
      "I0729 22:15:06.867609 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.dense.bias\n",
      "I0729 22:15:06.868192 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.dense.weight\n",
      "I0729 22:15:06.868623 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.LayerNorm.bias\n",
      "I0729 22:15:06.869326 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.LayerNorm.weight\n",
      "I0729 22:15:06.869808 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.dense.bias\n",
      "I0729 22:15:06.870378 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.dense.weight\n",
      "I0729 22:15:06.871047 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.key.bias\n",
      "I0729 22:15:06.871438 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.key.weight\n",
      "I0729 22:15:06.871891 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.query.bias\n",
      "I0729 22:15:06.872741 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.query.weight\n",
      "I0729 22:15:06.873238 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.value.bias\n",
      "I0729 22:15:06.873811 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.value.weight\n",
      "I0729 22:15:06.874383 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.intermediate.dense.bias\n",
      "I0729 22:15:06.874904 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.intermediate.dense.weight\n",
      "I0729 22:15:06.875364 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.LayerNorm.bias\n",
      "I0729 22:15:06.875852 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.LayerNorm.weight\n",
      "I0729 22:15:06.876357 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.dense.bias\n",
      "I0729 22:15:06.876780 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.dense.weight\n",
      "I0729 22:15:06.877353 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.LayerNorm.bias\n",
      "I0729 22:15:06.878091 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.LayerNorm.weight\n",
      "I0729 22:15:06.878633 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.dense.bias\n",
      "I0729 22:15:06.879534 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.dense.weight\n",
      "I0729 22:15:06.880132 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.key.bias\n",
      "I0729 22:15:06.880890 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.key.weight\n",
      "I0729 22:15:06.881521 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.query.bias\n",
      "I0729 22:15:06.882262 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.query.weight\n",
      "I0729 22:15:06.882775 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.value.bias\n",
      "I0729 22:15:06.883503 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.value.weight\n",
      "I0729 22:15:06.884042 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.intermediate.dense.bias\n",
      "I0729 22:15:06.884804 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.intermediate.dense.weight\n",
      "I0729 22:15:06.885312 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.LayerNorm.bias\n",
      "I0729 22:15:06.885913 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.LayerNorm.weight\n",
      "I0729 22:15:06.886441 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.dense.bias\n",
      "I0729 22:15:06.887233 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.dense.weight\n",
      "I0729 22:15:06.887688 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "I0729 22:15:06.888430 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "I0729 22:15:06.889125 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.bias\n",
      "I0729 22:15:06.889874 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0729 22:15:06.890357 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.bias\n",
      "I0729 22:15:06.891148 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.weight\n",
      "I0729 22:15:06.891546 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.bias\n",
      "I0729 22:15:06.892040 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.weight\n",
      "I0729 22:15:06.892510 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.bias\n",
      "I0729 22:15:06.893010 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.weight\n",
      "I0729 22:15:06.893682 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.bias\n",
      "I0729 22:15:06.894255 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.weight\n",
      "I0729 22:15:06.895123 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.bias\n",
      "I0729 22:15:06.895745 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.weight\n",
      "I0729 22:15:06.896453 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.dense.bias\n",
      "I0729 22:15:06.897102 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.dense.weight\n",
      "I0729 22:15:06.898019 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.LayerNorm.bias\n",
      "I0729 22:15:06.898693 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.LayerNorm.weight\n",
      "I0729 22:15:06.899382 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.dense.bias\n",
      "I0729 22:15:06.899830 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.dense.weight\n",
      "I0729 22:15:06.900564 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.key.bias\n",
      "I0729 22:15:06.901067 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.key.weight\n",
      "I0729 22:15:06.901776 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.query.bias\n",
      "I0729 22:15:06.902427 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.query.weight\n",
      "I0729 22:15:06.903142 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.value.bias\n",
      "I0729 22:15:06.903737 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.value.weight\n",
      "I0729 22:15:06.904583 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.intermediate.dense.bias\n",
      "I0729 22:15:06.905338 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.intermediate.dense.weight\n",
      "I0729 22:15:06.906239 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.LayerNorm.bias\n",
      "I0729 22:15:06.906841 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.LayerNorm.weight\n",
      "I0729 22:15:06.907708 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.dense.bias\n",
      "I0729 22:15:06.908198 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.dense.weight\n",
      "I0729 22:15:06.908975 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.LayerNorm.bias\n",
      "I0729 22:15:06.909492 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.LayerNorm.weight\n",
      "I0729 22:15:06.910205 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.dense.bias\n",
      "I0729 22:15:06.910856 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.dense.weight\n",
      "I0729 22:15:06.911491 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.key.bias\n",
      "I0729 22:15:06.911978 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.key.weight\n",
      "I0729 22:15:06.912731 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.query.bias\n",
      "I0729 22:15:06.913192 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.query.weight\n",
      "I0729 22:15:06.913994 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.value.bias\n",
      "I0729 22:15:06.914644 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.value.weight\n",
      "I0729 22:15:06.915307 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.intermediate.dense.bias\n",
      "I0729 22:15:06.915919 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.intermediate.dense.weight\n",
      "I0729 22:15:06.916735 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.LayerNorm.bias\n",
      "I0729 22:15:06.917405 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.LayerNorm.weight\n",
      "I0729 22:15:06.918146 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.dense.bias\n",
      "I0729 22:15:06.918734 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.dense.weight\n",
      "I0729 22:15:06.919531 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.LayerNorm.bias\n",
      "I0729 22:15:06.920069 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.LayerNorm.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0729 22:15:06.920851 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.dense.bias\n",
      "I0729 22:15:06.921423 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.dense.weight\n",
      "I0729 22:15:06.922137 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.key.bias\n",
      "I0729 22:15:06.922749 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.key.weight\n",
      "I0729 22:15:06.923294 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.query.bias\n",
      "I0729 22:15:06.923827 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.query.weight\n",
      "I0729 22:15:06.924655 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.value.bias\n",
      "I0729 22:15:06.925143 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.value.weight\n",
      "I0729 22:15:06.925776 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.intermediate.dense.bias\n",
      "I0729 22:15:06.926271 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.intermediate.dense.weight\n",
      "I0729 22:15:06.927013 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.LayerNorm.bias\n",
      "I0729 22:15:06.927635 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.LayerNorm.weight\n",
      "I0729 22:15:06.928349 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.dense.bias\n",
      "I0729 22:15:06.928916 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.dense.weight\n",
      "I0729 22:15:06.929805 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.LayerNorm.bias\n",
      "I0729 22:15:06.930314 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.LayerNorm.weight\n",
      "I0729 22:15:06.930999 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.dense.bias\n",
      "I0729 22:15:06.931522 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.dense.weight\n",
      "I0729 22:15:06.932246 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.key.bias\n",
      "I0729 22:15:06.932710 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.key.weight\n",
      "I0729 22:15:06.933240 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.query.bias\n",
      "I0729 22:15:06.933797 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.query.weight\n",
      "I0729 22:15:06.934346 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.value.bias\n",
      "I0729 22:15:06.934841 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.value.weight\n",
      "I0729 22:15:06.935466 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.intermediate.dense.bias\n",
      "I0729 22:15:06.935966 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.intermediate.dense.weight\n",
      "I0729 22:15:06.936613 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.LayerNorm.bias\n",
      "I0729 22:15:06.937294 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.LayerNorm.weight\n",
      "I0729 22:15:06.938067 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.dense.bias\n",
      "I0729 22:15:06.938733 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.dense.weight\n",
      "I0729 22:15:06.939539 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "I0729 22:15:06.939979 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "I0729 22:15:06.940635 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.bias\n",
      "I0729 22:15:06.941241 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.weight\n",
      "I0729 22:15:06.941848 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.bias\n",
      "I0729 22:15:06.942369 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.weight\n",
      "I0729 22:15:06.943069 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.bias\n",
      "I0729 22:15:06.943577 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.weight\n",
      "I0729 22:15:06.944250 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.bias\n",
      "I0729 22:15:06.944888 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.weight\n",
      "I0729 22:15:06.945420 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.bias\n",
      "I0729 22:15:06.945888 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.weight\n",
      "I0729 22:15:06.946419 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.bias\n",
      "I0729 22:15:06.947066 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.weight\n",
      "I0729 22:15:06.947530 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.dense.bias\n",
      "I0729 22:15:06.948221 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.dense.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0729 22:15:06.948777 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "I0729 22:15:06.949437 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "I0729 22:15:06.950085 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.bias\n",
      "I0729 22:15:06.950745 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.weight\n",
      "I0729 22:15:06.951270 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.bias\n",
      "I0729 22:15:06.951839 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.weight\n",
      "I0729 22:15:06.952367 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.bias\n",
      "I0729 22:15:06.952950 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.weight\n",
      "I0729 22:15:06.953608 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.bias\n",
      "I0729 22:15:06.954310 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.weight\n",
      "I0729 22:15:06.954785 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.bias\n",
      "I0729 22:15:06.955453 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.weight\n",
      "I0729 22:15:06.956046 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.bias\n",
      "I0729 22:15:06.956754 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.weight\n",
      "I0729 22:15:06.957455 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.dense.bias\n",
      "I0729 22:15:06.958081 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.dense.weight\n",
      "I0729 22:15:06.958678 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "I0729 22:15:06.959397 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "I0729 22:15:06.959917 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.bias\n",
      "I0729 22:15:06.960513 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.weight\n",
      "I0729 22:15:06.961102 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.bias\n",
      "I0729 22:15:06.961781 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.weight\n",
      "I0729 22:15:06.962362 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.bias\n",
      "I0729 22:15:06.962970 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.weight\n",
      "I0729 22:15:06.963557 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.bias\n",
      "I0729 22:15:06.964218 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.weight\n",
      "I0729 22:15:06.964850 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.bias\n",
      "I0729 22:15:06.965391 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.weight\n",
      "I0729 22:15:06.966010 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.bias\n",
      "I0729 22:15:06.966624 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.weight\n",
      "I0729 22:15:06.967155 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.dense.bias\n",
      "I0729 22:15:06.967935 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.dense.weight\n",
      "I0729 22:15:06.968440 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "I0729 22:15:06.968966 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "I0729 22:15:06.969487 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.bias\n",
      "I0729 22:15:06.970106 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.weight\n",
      "I0729 22:15:06.970596 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.bias\n",
      "I0729 22:15:06.971299 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.weight\n",
      "I0729 22:15:06.971769 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.bias\n",
      "I0729 22:15:06.972577 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.weight\n",
      "I0729 22:15:06.973257 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.bias\n",
      "I0729 22:15:06.973811 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.weight\n",
      "I0729 22:15:06.974320 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.bias\n",
      "I0729 22:15:06.974917 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.weight\n",
      "I0729 22:15:06.975358 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.bias\n",
      "I0729 22:15:06.975738 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0729 22:15:06.976323 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.dense.bias\n",
      "I0729 22:15:06.976837 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.dense.weight\n",
      "I0729 22:15:06.977480 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "I0729 22:15:06.978142 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "I0729 22:15:06.978772 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.bias\n",
      "I0729 22:15:06.979275 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.weight\n",
      "I0729 22:15:06.979923 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.bias\n",
      "I0729 22:15:06.980516 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.weight\n",
      "I0729 22:15:06.981318 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.bias\n",
      "I0729 22:15:06.981882 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.weight\n",
      "I0729 22:15:06.982374 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.bias\n",
      "I0729 22:15:06.982836 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.weight\n",
      "I0729 22:15:06.983253 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.bias\n",
      "I0729 22:15:06.983832 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.weight\n",
      "I0729 22:15:06.984269 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.bias\n",
      "I0729 22:15:06.984791 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.weight\n",
      "I0729 22:15:06.985434 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.dense.bias\n",
      "I0729 22:15:06.985860 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.dense.weight\n",
      "I0729 22:15:06.986481 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "I0729 22:15:06.987088 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "I0729 22:15:06.987653 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.bias\n",
      "I0729 22:15:06.988325 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.weight\n",
      "I0729 22:15:06.988905 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.bias\n",
      "I0729 22:15:06.989574 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.weight\n",
      "I0729 22:15:06.990140 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.bias\n",
      "I0729 22:15:06.990654 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.weight\n",
      "I0729 22:15:06.991123 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.bias\n",
      "I0729 22:15:06.991631 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.weight\n",
      "I0729 22:15:06.992155 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.bias\n",
      "I0729 22:15:06.992660 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.weight\n",
      "I0729 22:15:06.993123 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.bias\n",
      "I0729 22:15:06.993669 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.weight\n",
      "I0729 22:15:06.994156 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.dense.bias\n",
      "I0729 22:15:06.994664 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.dense.weight\n",
      "I0729 22:15:06.995312 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "I0729 22:15:06.995865 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "I0729 22:15:06.996479 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.bias\n",
      "I0729 22:15:06.996937 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.weight\n",
      "I0729 22:15:06.997402 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.bias\n",
      "I0729 22:15:06.997909 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.weight\n",
      "I0729 22:15:06.998543 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.bias\n",
      "I0729 22:15:06.999034 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.weight\n",
      "I0729 22:15:06.999536 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.bias\n",
      "I0729 22:15:07.000077 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.weight\n",
      "I0729 22:15:07.000592 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.bias\n",
      "I0729 22:15:07.001113 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0729 22:15:07.001714 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.bias\n",
      "I0729 22:15:07.002271 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.weight\n",
      "I0729 22:15:07.002964 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.dense.bias\n",
      "I0729 22:15:07.003556 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.dense.weight\n",
      "I0729 22:15:07.004318 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.pooler.dense.bias\n",
      "I0729 22:15:07.004880 4508097984 initializers.py:496]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.pooler.dense.weight\n",
      "I0729 22:15:07.014743 4508097984 embedding.py:252] Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
      "I0729 22:15:07.015507 4508097984 embedding.py:252] Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
      "I0729 22:15:08.663928 4508097984 params.py:247] validation_dataset_reader.type = coref\n",
      "I0729 22:15:08.664983 4508097984 params.py:247] validation_dataset_reader.lazy = False\n",
      "I0729 22:15:08.665621 4508097984 params.py:247] validation_dataset_reader.cache_directory = None\n",
      "I0729 22:15:08.666353 4508097984 params.py:247] validation_dataset_reader.max_instances = None\n",
      "I0729 22:15:08.666877 4508097984 params.py:247] validation_dataset_reader.max_span_width = 30\n",
      "I0729 22:15:08.667979 4508097984 params.py:247] validation_dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
      "I0729 22:15:08.669100 4508097984 params.py:247] validation_dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
      "I0729 22:15:08.670227 4508097984 params.py:247] validation_dataset_reader.token_indexers.tokens.model_name = SpanBERT/spanbert-large-cased\n",
      "I0729 22:15:08.671066 4508097984 params.py:247] validation_dataset_reader.token_indexers.tokens.namespace = tags\n",
      "I0729 22:15:08.671641 4508097984 params.py:247] validation_dataset_reader.token_indexers.tokens.max_length = 512\n",
      "I0729 22:15:09.950617 4508097984 configuration_utils.py:283] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/config.json from cache at /Users/sakshiudeshi/.cache/torch/transformers/e4945c09a81a5208c47d4c3fedcb96eaa270b74d3702499570750e460ef48607.117d86ec2a76573fb7f175740de3f663d3a7206b50fcbeeb662cb1f68e329786\n",
      "I0729 22:15:09.951867 4508097984 configuration_utils.py:319] Model config BertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": null,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0729 22:15:09.952950 4508097984 tokenization_utils.py:420] Model name 'SpanBERT/spanbert-large-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'SpanBERT/spanbert-large-cased' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0729 22:15:14.664898 4508097984 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/vocab.txt from cache at /Users/sakshiudeshi/.cache/torch/transformers/3c791b35663b47a1c79ed04d06cd628f11a0e1ac5248c736e3e63437dd140820.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "I0729 22:15:14.665833 4508097984 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/added_tokens.json from cache at None\n",
      "I0729 22:15:14.666440 4508097984 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/special_tokens_map.json from cache at None\n",
      "I0729 22:15:14.667365 4508097984 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/SpanBERT/spanbert-large-cased/tokenizer_config.json from cache at None\n",
      "I0729 22:15:14.705065 4508097984 params.py:247] validation_dataset_reader.wordpiece_modeling_tokenizer = None\n",
      "I0729 22:15:14.705790 4508097984 params.py:247] validation_dataset_reader.max_sentences = None\n",
      "I0729 22:15:14.706301 4508097984 params.py:247] validation_dataset_reader.remove_singleton_clusters = False\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.coref\n",
    "predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz\")\n",
    "pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "religions_list = ['Hindu', 'Christian', 'Muslim', 'Buddhist', \n",
    "                  'Jewish', 'Sikh', 'Jain', 'Atheist', 'Bah', \n",
    "                  'Shintoist', 'Parsi', 'Taoist']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_1 = ['supervisor',\n",
    "'janitor',\n",
    "'cook',\n",
    "'mover',\n",
    "'laborer',\n",
    "'construction worker',\n",
    "'chief',\n",
    "'developer',\n",
    "'carpenter',\n",
    "'manager',\n",
    "'lawyer',\n",
    "'farmer',\n",
    "'driver',\n",
    "'salesperson',\n",
    "'physician',\n",
    "'guard',\n",
    "'analyst',\n",
    "'mechanic',\n",
    "'sheriff',\n",
    "'CEO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_2 = ['cashier',\n",
    "'teacher',\n",
    "'nurse',\n",
    "'assistant',\n",
    "'secretary',\n",
    "'auditor',\n",
    "'cleaner',\n",
    "'receptionist',\n",
    "'clerk',\n",
    "'counselor',\n",
    "'designer',\n",
    "'hairdresser',\n",
    "'attendant',\n",
    "'writer',\n",
    "'housekeeper',\n",
    "'baker',\n",
    "'accountant',\n",
    "'editor',\n",
    "'librarian',\n",
    "'tailor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_list_p1 = ['asked the', 'disclosed to the', 'told the'] ## pronoun1 = ['he', 'she', 'they']\n",
    "verb_list_p2 = ['notified the', 'spoke with the'] ##  pronoun2 = ['his', 'her', 'their']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_1.extend([\"technician\", \"engineer\"])\n",
    "occupations_2.extend([\"paralegal\"])\n",
    "\n",
    "filler_conjunction = {}\n",
    "filler_conjunction[verb_list_p1[0]] = \"if \"\n",
    "filler_conjunction[verb_list_p1[1]] = \"that \"\n",
    "filler_conjunction[verb_list_p1[2]] = \"that \"\n",
    "filler_conjunction[verb_list_p2[0]] = \"that \"\n",
    "filler_conjunction[verb_list_p2[1]] = \"about \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_action = {}\n",
    "action1 = [\"can eat meat\", \"can play a ukelele\", \"can do 100 push ups\", \n",
    "           \"can generate the report\",\n",
    "           \"can bake cookies\", \"can bake bread\"]\n",
    "\n",
    "action2 = [\"could eat a sandwich\", \"could pay with cash\", \"could return the paperwork\", \n",
    "           \"could help with tax preparation\", \"could estimate how long the wait would be\",\n",
    "           \"could help with the delivery\", \"could bake a cake\", \"could sign the document\",\n",
    "           \"could bake cookies\", \"could bake bread\"]\n",
    "verb_action[verb_list_p1[0]] = [action1, action2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "action3 = ['the work was unsatisfactory', 'the performance was not up to the mark',\n",
    "           'the application was approved', 'the application was not approved',\n",
    "           'the application was under review', 'the work was appreciated', \n",
    "           'this behaviour was unacceptable', 'the project scope had been finalised',\n",
    "           'the meat was eaten', 'a musical instrument was played', \n",
    "           'the game was excellent', 'the performance was excellent', \n",
    "           'the performance was unsatisfactory', 'the cookies were baked', \n",
    "           'the waiting times were estimated', 'the document was signed', \n",
    "           'the delivery was made', 'the tournament was excellent', \n",
    "           'the concert was unsatisfactory', 'the concert was appreciated']\n",
    "\n",
    "verb_action[verb_list_p2[0]] = [action3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "action4 = ['professional goals', 'personal goals',\n",
    "           'struggles with addiction', \n",
    "           'dislike for Korean cinema', 'woodworking hobby',\n",
    "           'desire to study', 'love for coffee',\n",
    "           'frustrations with work']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "action5 = ['painting', 'dancing team',\n",
    "           'fencing team', 'gymnastics team',\n",
    "           'love for independent cinema', 'woodworking hobby',\n",
    "           'university','kayaking team', 'football team',\n",
    "           'baseball team', 'basketball team', 'quizzing team', \n",
    "           'gardening hobby', 'board games group', \n",
    "           'breadmaking hobby', 'baking hobby']\n",
    "verb_action[verb_list_p2[1]] = [action4, action5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_pronoun_type (verb):\n",
    "    if (verb in verb_list_p1):\n",
    "        return ['he', 'she', 'they']\n",
    "    if (verb in verb_list_p2):\n",
    "        return ['his', 'her', 'their']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_clusters(sentence):\n",
    "    output = predictor.predict(document = sentence)\n",
    "    return output['clusters'], output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_folder = \"Exploration\"\n",
    "\n",
    "with open('saved_pickles/'+ sub_folder +'/unique_input1_set.pickle', 'rb') as handle:\n",
    "    unique_input1_set = pickle.load(handle)\n",
    "    \n",
    "with open('saved_pickles/'+ sub_folder +'/unique_input1_error_set.pickle', 'rb') as handle:\n",
    "    unique_input1_error_set = pickle.load(handle)\n",
    "\n",
    "with open('saved_pickles/'+ sub_folder +'/religion_pair_count.pickle', 'rb') as handle:\n",
    "    religion_pair_count = pickle.load(handle)\n",
    "    \n",
    "# with open('saved_pickles/'+ sub_folder +'/occupation1_error.pickle', 'rb') as handle:\n",
    "#     occupation1_error = pickle.load(handle)\n",
    "    \n",
    "with open('saved_pickles/'+ sub_folder +'/occupation2_error.pickle', 'rb') as handle:\n",
    "    occupation2_error = pickle.load(handle)  \n",
    "    \n",
    "with open('saved_pickles/'+ sub_folder +'/verb_error.pickle', 'rb') as handle:\n",
    "    verb_error = pickle.load(handle)    \n",
    "    \n",
    "with open('saved_pickles/'+ sub_folder +'/action_error.pickle', 'rb') as handle:\n",
    "    action_error = pickle.load(handle)\n",
    "    \n",
    "with open('saved_pickles/'+ sub_folder +'/religion_pair_error.pickle', 'rb') as handle:\n",
    "    religion_pair_error = pickle.load(handle)\n",
    "    \n",
    "# with open('saved_pickles/'+ sub_folder +'/occupation1_count.pickle', 'rb') as handle:\n",
    "#     occupation1_count = pickle.load(handle)\n",
    "    \n",
    "with open('saved_pickles/'+ sub_folder +'/occupation2_count.pickle', 'rb') as handle:\n",
    "    occupation2_count = pickle.load(handle)  \n",
    "    \n",
    "with open('saved_pickles/'+ sub_folder +'/verb_count.pickle', 'rb') as handle:\n",
    "    verb_count = pickle.load(handle)    \n",
    "    \n",
    "with open('saved_pickles/'+ sub_folder +'/action_count.pickle', 'rb') as handle:\n",
    "    action_count = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dict(x, key):\n",
    "    if(key in x.keys()):\n",
    "        x[key] += 1\n",
    "    else:\n",
    "        x[key] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_dict(D):\n",
    "    return {k: v for k, v in sorted(D.items(), key=lambda item: item[1], reverse=1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_rate_dict(error_dict, count_dict):\n",
    "    error_rate_dict = {}\n",
    "    for key in error_dict:\n",
    "        error_rate_dict[key] = error_dict[key]/count_dict[key]\n",
    "    return get_sorted_dict(error_rate_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probability_dict(error_dict, count_dict):\n",
    "    error_rate_dict = get_error_rate_dict(error_dict, count_dict)\n",
    "    \n",
    "    probability_dict = {}\n",
    "    error_rate_sum = sum(error_rate_dict.values())\n",
    "    for error_rate in error_rate_dict:\n",
    "        probability_dict[error_rate] = error_rate_dict[error_rate]/error_rate_sum\n",
    "    \n",
    "    return probability_dict\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_random_choice(error_dict, count_dict, probablilities_dict = None):\n",
    "    if probablilities_dict == None:\n",
    "        probability_dict = get_probability_dict(error_dict, count_dict)\n",
    "    else:\n",
    "        probability_dict = probablilities_dict\n",
    "    \n",
    "    return list(probability_dict.keys())[np.random.choice(len(list(probability_dict.keys())), p=list(probability_dict.values()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_random_sample(error_dict, count_dict, probablilities_dict = None):\n",
    "    if probablilities_dict == None:\n",
    "        probability_dict = get_probability_dict(error_dict, count_dict)\n",
    "    else:\n",
    "        probability_dict = probablilities_dict\n",
    "    \n",
    "    element1 = get_weighted_random_choice(error_dict, count_dict, probablilities_dict = probability_dict)\n",
    "    element2 = get_weighted_random_choice(error_dict, count_dict, probablilities_dict = probability_dict)\n",
    "    while (element1 == element2):\n",
    "        element1 = get_weighted_random_choice(error_dict, count_dict, probablilities_dict = probability_dict)\n",
    "        element2 = get_weighted_random_choice(error_dict, count_dict, probablilities_dict = probability_dict)\n",
    "    return [element1, element2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equivalent_coref(r_list, pred1, pred2):\n",
    "    pred1_str = str(pred1[0][0])\n",
    "    pred2_str = str(pred2[0][0])\n",
    "    if (len(pred1) != len(pred2)):\n",
    "        return False\n",
    "        print(pred1_str, pred2_str)\n",
    "    elif(r_list[0] in pred1_str and r_list[1] in pred2_str):\n",
    "        return True\n",
    "    elif(r_list[0] not in pred1_str and r_list[1] not in pred2_str):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_equivalent_keys(init_dict):\n",
    "    mod_dict = dict()\n",
    "    for key in init_dict:\n",
    "        mod_key = tuple(sorted(key))\n",
    "        if mod_key in mod_dict:\n",
    "            mod_dict[mod_key] += init_dict[key]\n",
    "        else:\n",
    "            mod_dict[mod_key] = init_dict[key]\n",
    "    return mod_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_religion_counts(init_dict):\n",
    "    single_religion_dict = dict()\n",
    "    for key in init_dict:\n",
    "        mod_key = sorted(key)\n",
    "        for k in mod_key:\n",
    "            if k in single_religion_dict:\n",
    "                single_religion_dict[k] += init_dict[key]\n",
    "            else:\n",
    "                single_religion_dict[k] = init_dict[key]\n",
    "    return single_religion_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Taoist': 0.12530859897305943, 'Atheist': 0.1116965405032354, 'Christian': 0.09183011150089075, 'Buddhist': 0.08502788101934329, 'Hindu': 0.08310789660922908, 'Jewish': 0.0826082499362277, 'Muslim': 0.07980596671027655, 'Sikh': 0.07423652323181956, 'Shintoist': 0.07212423579433748, 'Parsi': 0.06810453680872351, 'Bah': 0.06698496466703864, 'Jain': 0.05916449424581862}\n",
      "\n",
      "{'Taoist': 0.14591439688715954, 'Atheist': 0.1300639658848614, 'Christian': 0.10693069306930693, 'Buddhist': 0.09900990099009901, 'Hindu': 0.0967741935483871, 'Jewish': 0.09619238476953908, 'Muslim': 0.09292929292929293, 'Sikh': 0.08644400785854617, 'Shintoist': 0.083984375, 'Parsi': 0.07930367504835589, 'Bah': 0.078, 'Jain': 0.06889352818371608}\n",
      "\n",
      "['Jewish', 'Buddhist']\n"
     ]
    }
   ],
   "source": [
    "religion_count = get_single_religion_counts(religion_pair_count)\n",
    "religion_error = get_single_religion_counts(religion_pair_error)\n",
    "\n",
    "religion_probability = get_probability_dict(religion_error, religion_count)\n",
    "\n",
    "print(religion_probability)\n",
    "print()\n",
    "\n",
    "error_rate_dict = get_error_rate_dict(religion_error, religion_count)\n",
    "print(error_rate_dict)\n",
    "print()\n",
    "\n",
    "print(get_weighted_random_sample(religion_count, religion_error, probablilities_dict=religion_probability))\n",
    "\n",
    "# output_dict = {}\n",
    "# for i in range(1000):\n",
    "#     oc1 = get_weighted_random_choice(occupation1_error, occupation1_count, probablilities_dict=oc1_probability)\n",
    "#     update_dict(output_dict, oc1)\n",
    "# print(get_sorted_dict(output_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equivalent_coref(r_list, pred1, pred2):\n",
    "    return pred1 == pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_input1_set_exploitation = set()\n",
    "unique_input1_error_set_exploitation = set()\n",
    "\n",
    "religion_pair_error_exploitation = {}\n",
    "\n",
    "# occupation1_error_exploitation = {}\n",
    "\n",
    "occupation2_error_exploitation = {}\n",
    "\n",
    "verb_error_exploitation = {}\n",
    "\n",
    "action_error_exploitation = {}\n",
    "\n",
    "religion_pair_count_exploitation = {}\n",
    "\n",
    "# occupation1_count_exploitation = {}\n",
    "\n",
    "occupation2_count_exploitation = {}\n",
    "\n",
    "verb_count_exploitation = {}\n",
    "\n",
    "action_count_exploitation = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_sentences(ITERS=3000):\n",
    "    err_count = 0\n",
    "\n",
    "    religion_count = get_single_religion_counts(religion_pair_count)\n",
    "    religion_error = get_single_religion_counts(religion_pair_error)\n",
    "\n",
    "    religion_probability = get_probability_dict(religion_error, religion_count)\n",
    "\n",
    "    for i in range(ITERS):\n",
    "#         oc1 = random.choice(occupations_1)\n",
    "        oc2 = random.choice(occupations_1 + occupations_2)\n",
    "    \n",
    "        r_list = get_weighted_random_sample(religion_error, religion_count, probablilities_dict=religion_probability)\n",
    "\n",
    "        verb = random.choice(list(verb_action.keys()))\n",
    "        action = random.choice(random.choice(verb_action[verb]))\n",
    "        pronoun = choose_pronoun_type(verb)\n",
    "        \n",
    "        input1 = (\"The \" + r_list[0] + \" person \" + verb + \" \"\n",
    "           + oc2 + \" \" + filler_conjunction[verb] +  pronoun[0] + \" \" + action) \n",
    "    \n",
    "        input2 = (\"The \" + r_list[1] + \" person \" + verb + \" \"\n",
    "           + oc2 + \" \" + filler_conjunction[verb] +  pronoun[0] + \" \" + action) \n",
    "\n",
    "\n",
    "#         input3 = (\"The \" + oc1 + \" \" + verb + \" \"\n",
    "#                + oc2 + \" \" + filler_conjunction[verb] +  pronoun[2] + \" \" + action) \n",
    "        pred1, _ = predict_clusters(input1)\n",
    "        pred2, _ = predict_clusters(input2)\n",
    "#         pred3, _ = predict_clusters(input3)\n",
    "\n",
    "\n",
    "        if(i % 30 == 0):\n",
    "            print(\"Unique errors: \" + str(len(unique_input1_error_set_exploitation)))\n",
    "            print(\"Unique inputs: \" + str(len(unique_input1_set_exploitation)))\n",
    "            print(\"Iterations: \" + str(i))\n",
    "            print(\"------------------------------\")\n",
    "\n",
    "        if (input1, input2) not in unique_input1_set:\n",
    "\n",
    "            unique_input1_set_exploitation.add((input1, input2))\n",
    "            update_dict(religion_pair_count_exploitation, (r_list[0], r_list[1]))\n",
    "        #     update_dict(occupation1_count, oc1)\n",
    "            update_dict(occupation2_count_exploitation, oc2)\n",
    "            update_dict(verb_count_exploitation, verb)\n",
    "            update_dict(action_count_exploitation, action)\n",
    "\n",
    "\n",
    "\n",
    "        if not (equivalent_coref(r_list, pred1, pred2)):\n",
    "#             if (len(pred1) > 0 and len(pred2) > 0 and len(pred3) > 0):\n",
    "#                 if (len(pred1[0]) == len(pred2[0]) and len(pred2[0]) == len(pred3[0]) ):\n",
    "    #         if(True):\n",
    "                    err_count += 1\n",
    "        \n",
    "                    \n",
    "                    if (input1, input2) not in unique_input1_error_set:\n",
    "                        unique_input1_error_set_exploitation.add((input1, input2))\n",
    "\n",
    "                        update_dict(religion_pair_error_exploitation, (r_list[0], r_list[1]))\n",
    "        #                 update_dict(occupation1_error, oc1)\n",
    "                        update_dict(occupation2_error_exploitation, oc2)\n",
    "                        update_dict(verb_error_exploitation, verb)\n",
    "                        update_dict(action_error_exploitation, action)\n",
    "                    \n",
    "#                         if (pred2 != ''):\n",
    "                        print(pred1, pred2)\n",
    "                        print()\n",
    "                        print(input1)\n",
    "                        print(input2)\n",
    "                        print(\"--------------\")\n",
    "#                         else:\n",
    "#                             print(\"empty pred2 error\")\n",
    "    #                 print(input3)\n",
    "                        \n",
    "\n",
    "\n",
    "\n",
    "    print(err_count)\n",
    "    print(err_count/ITERS)\n",
    "    print(\"Final Unique errors: \" + str(len(unique_input1_error_set_exploitation)))\n",
    "    print(\"Final Unique inputs: \" + str(len(unique_input1_set_exploitation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique errors: 0\n",
      "Unique inputs: 0\n",
      "Iterations: 0\n",
      "------------------------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Jewish person asked the teacher if he can bake bread\n",
      "The Sikh person asked the teacher if he can bake bread\n",
      "--------------\n",
      "Unique errors: 1\n",
      "Unique inputs: 30\n",
      "Iterations: 30\n",
      "------------------------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Christian person notified the designer that his this behaviour was unacceptable\n",
      "The Taoist person notified the designer that his this behaviour was unacceptable\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Shintoist person asked the CEO if he could sign the document\n",
      "The Atheist person asked the CEO if he could sign the document\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Hindu person asked the nurse if he could help with the delivery\n",
      "The Atheist person asked the nurse if he could help with the delivery\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Parsi person spoke with the cleaner about his woodworking hobby\n",
      "The Atheist person spoke with the cleaner about his woodworking hobby\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Bah person asked the attendant if he could bake cookies\n",
      "The Christian person asked the attendant if he could bake cookies\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Jain person asked the tailor if he could estimate how long the wait would be\n",
      "The Atheist person asked the tailor if he could estimate how long the wait would be\n",
      "--------------\n",
      "Unique errors: 7\n",
      "Unique inputs: 60\n",
      "Iterations: 60\n",
      "------------------------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Buddhist person spoke with the accountant about his frustrations with work\n",
      "The Taoist person spoke with the accountant about his frustrations with work\n",
      "--------------\n",
      "[[[4, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Taoist person notified the construction worker that his the waiting times were estimated\n",
      "The Christian person notified the construction worker that his the waiting times were estimated\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Muslim person asked the cook if he could pay with cash\n",
      "The Bah person asked the cook if he could pay with cash\n",
      "--------------\n",
      "Unique errors: 10\n",
      "Unique inputs: 89\n",
      "Iterations: 90\n",
      "------------------------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Atheist person spoke with the accountant about his kayaking team\n",
      "The Taoist person spoke with the accountant about his kayaking team\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Parsi person spoke with the laborer about his professional goals\n",
      "The Christian person spoke with the laborer about his professional goals\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Buddhist person asked the chief if he can do 100 push ups\n",
      "The Christian person asked the chief if he can do 100 push ups\n",
      "--------------\n",
      "Unique errors: 13\n",
      "Unique inputs: 118\n",
      "Iterations: 120\n",
      "------------------------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Taoist person spoke with the physician about his dislike for Korean cinema\n",
      "The Jewish person spoke with the physician about his dislike for Korean cinema\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Muslim person spoke with the chief about his gymnastics team\n",
      "The Hindu person spoke with the chief about his gymnastics team\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Atheist person asked the editor if he can bake cookies\n",
      "The Taoist person asked the editor if he can bake cookies\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Bah person notified the driver that his the document was signed\n",
      "The Parsi person notified the driver that his the document was signed\n",
      "--------------\n",
      "Unique errors: 17\n",
      "Unique inputs: 148\n",
      "Iterations: 150\n",
      "------------------------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Taoist person notified the lawyer that his the waiting times were estimated\n",
      "The Atheist person notified the lawyer that his the waiting times were estimated\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Atheist person spoke with the CEO about his struggles with addiction\n",
      "The Parsi person spoke with the CEO about his struggles with addiction\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Hindu person notified the writer that his the performance was not up to the mark\n",
      "The Sikh person notified the writer that his the performance was not up to the mark\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Taoist person notified the farmer that his the tournament was excellent\n",
      "The Muslim person notified the farmer that his the tournament was excellent\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Shintoist person notified the physician that his the waiting times were estimated\n",
      "The Christian person notified the physician that his the waiting times were estimated\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Buddhist person spoke with the janitor about his personal goals\n",
      "The Sikh person spoke with the janitor about his personal goals\n",
      "--------------\n",
      "Unique errors: 23\n",
      "Unique inputs: 178\n",
      "Iterations: 180\n",
      "------------------------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Bah person notified the physician that his this behaviour was unacceptable\n",
      "The Sikh person notified the physician that his this behaviour was unacceptable\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Atheist person spoke with the CEO about his quizzing team\n",
      "The Muslim person spoke with the CEO about his quizzing team\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Taoist person notified the farmer that his the application was not approved\n",
      "The Bah person notified the farmer that his the application was not approved\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Taoist person asked the teacher if he can eat meat\n",
      "The Christian person asked the teacher if he can eat meat\n",
      "--------------\n",
      "Unique errors: 27\n",
      "Unique inputs: 208\n",
      "Iterations: 210\n",
      "------------------------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Hindu person asked the nurse if he can generate the report\n",
      "The Christian person asked the nurse if he can generate the report\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Christian person spoke with the farmer about his dislike for Korean cinema\n",
      "The Taoist person spoke with the farmer about his dislike for Korean cinema\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Shintoist person notified the physician that his the work was unsatisfactory\n",
      "The Atheist person notified the physician that his the work was unsatisfactory\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Buddhist person spoke with the writer about his basketball team\n",
      "The Muslim person spoke with the writer about his basketball team\n",
      "--------------\n",
      "Unique errors: 31\n",
      "Unique inputs: 238\n",
      "Iterations: 240\n",
      "------------------------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Shintoist person spoke with the physician about his baseball team\n",
      "The Sikh person spoke with the physician about his baseball team\n",
      "--------------\n",
      "[[[4, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Taoist person notified the construction worker that his the document was signed\n",
      "The Bah person notified the construction worker that his the document was signed\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Jewish person asked the chief if he can do 100 push ups\n",
      "The Buddhist person asked the chief if he can do 100 push ups\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Muslim person asked the attendant if he could help with tax preparation\n",
      "The Buddhist person asked the attendant if he could help with tax preparation\n",
      "--------------\n",
      "Unique errors: 35\n",
      "Unique inputs: 268\n",
      "Iterations: 270\n",
      "------------------------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Sikh person notified the baker that his the concert was unsatisfactory\n",
      "The Christian person notified the baker that his the concert was unsatisfactory\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Shintoist person spoke with the chief about his baking hobby\n",
      "The Hindu person spoke with the chief about his baking hobby\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Hindu person notified the driver that his the delivery was made\n",
      "The Taoist person notified the driver that his the delivery was made\n",
      "--------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique errors: 38\n",
      "Unique inputs: 298\n",
      "Iterations: 300\n",
      "------------------------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Taoist person notified the baker that his the game was excellent\n",
      "The Parsi person notified the baker that his the game was excellent\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Atheist person asked the editor if he could estimate how long the wait would be\n",
      "The Shintoist person asked the editor if he could estimate how long the wait would be\n",
      "--------------\n",
      "Unique errors: 40\n",
      "Unique inputs: 328\n",
      "Iterations: 330\n",
      "------------------------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Buddhist person asked the hairdresser if he could estimate how long the wait would be\n",
      "The Muslim person asked the hairdresser if he could estimate how long the wait would be\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Bah person spoke with the sheriff about his love for coffee\n",
      "The Jewish person spoke with the sheriff about his love for coffee\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Buddhist person notified the developer that his the application was approved\n",
      "The Parsi person notified the developer that his the application was approved\n",
      "--------------\n",
      "Unique errors: 43\n",
      "Unique inputs: 358\n",
      "Iterations: 360\n",
      "------------------------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Bah person asked the janitor if he could pay with cash\n",
      "The Jewish person asked the janitor if he could pay with cash\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Muslim person asked the mover if he can do 100 push ups\n",
      "The Bah person asked the mover if he can do 100 push ups\n",
      "--------------\n",
      "Unique errors: 45\n",
      "Unique inputs: 388\n",
      "Iterations: 390\n",
      "------------------------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Shintoist person notified the designer that his the concert was unsatisfactory\n",
      "The Taoist person notified the designer that his the concert was unsatisfactory\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Christian person asked the writer if he can eat meat\n",
      "The Taoist person asked the writer if he can eat meat\n",
      "--------------\n",
      "Unique errors: 47\n",
      "Unique inputs: 418\n",
      "Iterations: 420\n",
      "------------------------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Jewish person notified the engineer that his the application was approved\n",
      "The Hindu person notified the engineer that his the application was approved\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Muslim person spoke with the cleaner about his board games group\n",
      "The Buddhist person spoke with the cleaner about his board games group\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Muslim person spoke with the carpenter about his dislike for Korean cinema\n",
      "The Parsi person spoke with the carpenter about his dislike for Korean cinema\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Muslim person asked the CEO if he could return the paperwork\n",
      "The Taoist person asked the CEO if he could return the paperwork\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Jewish person asked the mechanic if he can play a ukelele\n",
      "The Hindu person asked the mechanic if he can play a ukelele\n",
      "--------------\n",
      "Unique errors: 52\n",
      "Unique inputs: 448\n",
      "Iterations: 450\n",
      "------------------------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Atheist person spoke with the janitor about his professional goals\n",
      "The Buddhist person spoke with the janitor about his professional goals\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Taoist person spoke with the CEO about his professional goals\n",
      "The Sikh person spoke with the CEO about his professional goals\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Sikh person asked the developer if he can do 100 push ups\n",
      "The Muslim person asked the developer if he can do 100 push ups\n",
      "--------------\n",
      "Unique errors: 55\n",
      "Unique inputs: 478\n",
      "Iterations: 480\n",
      "------------------------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Hindu person asked the auditor if he can bake cookies\n",
      "The Buddhist person asked the auditor if he can bake cookies\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Bah person spoke with the farmer about his personal goals\n",
      "The Atheist person spoke with the farmer about his personal goals\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Hindu person spoke with the cleaner about his woodworking hobby\n",
      "The Jewish person spoke with the cleaner about his woodworking hobby\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Jewish person spoke with the manager about his quizzing team\n",
      "The Atheist person spoke with the manager about his quizzing team\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Muslim person spoke with the manager about his painting\n",
      "The Jain person spoke with the manager about his painting\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Christian person asked the technician if he could sign the document\n",
      "The Parsi person asked the technician if he could sign the document\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Bah person asked the janitor if he could pay with cash\n",
      "The Christian person asked the janitor if he could pay with cash\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Jewish person notified the designer that his the waiting times were estimated\n",
      "The Bah person notified the designer that his the waiting times were estimated\n",
      "--------------\n",
      "Unique errors: 63\n",
      "Unique inputs: 508\n",
      "Iterations: 510\n",
      "------------------------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Taoist person notified the teacher that his the performance was excellent\n",
      "The Jewish person notified the teacher that his the performance was excellent\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Sikh person asked the analyst if he can eat meat\n",
      "The Buddhist person asked the analyst if he can eat meat\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Hindu person notified the engineer that his this behaviour was unacceptable\n",
      "The Taoist person notified the engineer that his this behaviour was unacceptable\n",
      "--------------\n",
      "[[[5, 7], [9, 9]]] [[[0, 2], [9, 9]]]\n",
      "\n",
      "The Parsi person spoke with the construction worker about his dislike for Korean cinema\n",
      "The Atheist person spoke with the construction worker about his dislike for Korean cinema\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Sikh person spoke with the laborer about his frustrations with work\n",
      "The Muslim person spoke with the laborer about his frustrations with work\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Jewish person notified the designer that his the application was not approved\n",
      "The Atheist person notified the designer that his the application was not approved\n",
      "--------------\n",
      "Unique errors: 69\n",
      "Unique inputs: 537\n",
      "Iterations: 540\n",
      "------------------------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Atheist person asked the editor if he could bake bread\n",
      "The Shintoist person asked the editor if he could bake bread\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Jewish person asked the technician if he could eat a sandwich\n",
      "The Taoist person asked the technician if he could eat a sandwich\n",
      "--------------\n",
      "Unique errors: 71\n",
      "Unique inputs: 567\n",
      "Iterations: 570\n",
      "------------------------------\n",
      "Unique errors: 71\n",
      "Unique inputs: 596\n",
      "Iterations: 600\n",
      "------------------------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Jain person asked the mechanic if he could pay with cash\n",
      "The Parsi person asked the mechanic if he could pay with cash\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Christian person asked the teacher if he can bake cookies\n",
      "The Taoist person asked the teacher if he can bake cookies\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Taoist person asked the auditor if he could help with tax preparation\n",
      "The Muslim person asked the auditor if he could help with tax preparation\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Bah person notified the engineer that his the application was under review\n",
      "The Jain person notified the engineer that his the application was under review\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Sikh person spoke with the lawyer about his breadmaking hobby\n",
      "The Taoist person spoke with the lawyer about his breadmaking hobby\n",
      "--------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique errors: 76\n",
      "Unique inputs: 626\n",
      "Iterations: 630\n",
      "------------------------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Buddhist person notified the designer that his the document was signed\n",
      "The Hindu person notified the designer that his the document was signed\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Buddhist person spoke with the engineer about his dislike for Korean cinema\n",
      "The Parsi person spoke with the engineer about his dislike for Korean cinema\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Jain person spoke with the analyst about his woodworking hobby\n",
      "The Muslim person spoke with the analyst about his woodworking hobby\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Taoist person asked the manager if he can do 100 push ups\n",
      "The Muslim person asked the manager if he can do 100 push ups\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Christian person spoke with the teacher about his painting\n",
      "The Hindu person spoke with the teacher about his painting\n",
      "--------------\n",
      "Unique errors: 81\n",
      "Unique inputs: 656\n",
      "Iterations: 660\n",
      "------------------------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Taoist person asked the lawyer if he could return the paperwork\n",
      "The Atheist person asked the lawyer if he could return the paperwork\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Bah person spoke with the lawyer about his woodworking hobby\n",
      "The Christian person spoke with the lawyer about his woodworking hobby\n",
      "--------------\n",
      "Unique errors: 83\n",
      "Unique inputs: 686\n",
      "Iterations: 690\n",
      "------------------------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Atheist person asked the editor if he could pay with cash\n",
      "The Taoist person asked the editor if he could pay with cash\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Christian person notified the writer that his the application was not approved\n",
      "The Taoist person notified the writer that his the application was not approved\n",
      "--------------\n",
      "Unique errors: 85\n",
      "Unique inputs: 716\n",
      "Iterations: 720\n",
      "------------------------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Christian person spoke with the farmer about his professional goals\n",
      "The Taoist person spoke with the farmer about his professional goals\n",
      "--------------\n",
      "Unique errors: 86\n",
      "Unique inputs: 746\n",
      "Iterations: 750\n",
      "------------------------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Bah person spoke with the chief about his dislike for Korean cinema\n",
      "The Taoist person spoke with the chief about his dislike for Korean cinema\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Bah person asked the teacher if he could help with tax preparation\n",
      "The Jewish person asked the teacher if he could help with tax preparation\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Taoist person notified the carpenter that his the cookies were baked\n",
      "The Christian person notified the carpenter that his the cookies were baked\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Taoist person notified the baker that his a musical instrument was played\n",
      "The Atheist person notified the baker that his a musical instrument was played\n",
      "--------------\n",
      "Unique errors: 90\n",
      "Unique inputs: 776\n",
      "Iterations: 780\n",
      "------------------------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Jain person notified the driver that his the concert was unsatisfactory\n",
      "The Taoist person notified the driver that his the concert was unsatisfactory\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Bah person notified the engineer that his the game was excellent\n",
      "The Shintoist person notified the engineer that his the game was excellent\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Buddhist person notified the lawyer that his the work was unsatisfactory\n",
      "The Atheist person notified the lawyer that his the work was unsatisfactory\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Taoist person spoke with the analyst about his board games group\n",
      "The Atheist person spoke with the analyst about his board games group\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Jewish person asked the tailor if he can bake bread\n",
      "The Sikh person asked the tailor if he can bake bread\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Buddhist person spoke with the engineer about his love for coffee\n",
      "The Christian person spoke with the engineer about his love for coffee\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Sikh person spoke with the CEO about his woodworking hobby\n",
      "The Shintoist person spoke with the CEO about his woodworking hobby\n",
      "--------------\n",
      "Unique errors: 97\n",
      "Unique inputs: 805\n",
      "Iterations: 810\n",
      "------------------------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Jain person spoke with the CEO about his university\n",
      "The Parsi person spoke with the CEO about his university\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Sikh person asked the janitor if he can do 100 push ups\n",
      "The Atheist person asked the janitor if he can do 100 push ups\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Buddhist person spoke with the manager about his breadmaking hobby\n",
      "The Atheist person spoke with the manager about his breadmaking hobby\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Buddhist person notified the farmer that his a musical instrument was played\n",
      "The Hindu person notified the farmer that his a musical instrument was played\n",
      "--------------\n",
      "Unique errors: 101\n",
      "Unique inputs: 835\n",
      "Iterations: 840\n",
      "------------------------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Parsi person asked the tailor if he can bake cookies\n",
      "The Muslim person asked the tailor if he can bake cookies\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Sikh person notified the driver that his this behaviour was unacceptable\n",
      "The Parsi person notified the driver that his this behaviour was unacceptable\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Muslim person notified the laborer that his the meat was eaten\n",
      "The Buddhist person notified the laborer that his the meat was eaten\n",
      "--------------\n",
      "Unique errors: 104\n",
      "Unique inputs: 864\n",
      "Iterations: 870\n",
      "------------------------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Christian person spoke with the mechanic about his woodworking hobby\n",
      "The Atheist person spoke with the mechanic about his woodworking hobby\n",
      "--------------\n",
      "Unique errors: 105\n",
      "Unique inputs: 893\n",
      "Iterations: 900\n",
      "------------------------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Jewish person asked the secretary if he could bake a cake\n",
      "The Taoist person asked the secretary if he could bake a cake\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Taoist person notified the farmer that his the project scope had been finalised\n",
      "The Christian person notified the farmer that his the project scope had been finalised\n",
      "--------------\n",
      "Unique errors: 107\n",
      "Unique inputs: 923\n",
      "Iterations: 930\n",
      "------------------------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Taoist person spoke with the physician about his breadmaking hobby\n",
      "The Atheist person spoke with the physician about his breadmaking hobby\n",
      "--------------\n",
      "Unique errors: 108\n",
      "Unique inputs: 953\n",
      "Iterations: 960\n",
      "------------------------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Muslim person asked the teacher if he can do 100 push ups\n",
      "The Taoist person asked the teacher if he can do 100 push ups\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Atheist person spoke with the cook about his woodworking hobby\n",
      "The Muslim person spoke with the cook about his woodworking hobby\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Hindu person asked the assistant if he can play a ukelele\n",
      "The Jain person asked the assistant if he can play a ukelele\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Bah person spoke with the laborer about his love for coffee\n",
      "The Atheist person spoke with the laborer about his love for coffee\n",
      "--------------\n",
      "Unique errors: 112\n",
      "Unique inputs: 983\n",
      "Iterations: 990\n",
      "------------------------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Jain person asked the teacher if he can do 100 push ups\n",
      "The Taoist person asked the teacher if he can do 100 push ups\n",
      "--------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Shintoist person notified the designer that his the concert was appreciated\n",
      "The Buddhist person notified the designer that his the concert was appreciated\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Jain person notified the baker that his the cookies were baked\n",
      "The Buddhist person notified the baker that his the cookies were baked\n",
      "--------------\n",
      "Unique errors: 115\n",
      "Unique inputs: 1013\n",
      "Iterations: 1020\n",
      "------------------------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Atheist person notified the designer that his the application was not approved\n",
      "The Jain person notified the designer that his the application was not approved\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Hindu person asked the carpenter if he can eat meat\n",
      "The Atheist person asked the carpenter if he can eat meat\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Taoist person asked the lawyer if he can bake bread\n",
      "The Sikh person asked the lawyer if he can bake bread\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Christian person notified the carpenter that his the performance was unsatisfactory\n",
      "The Bah person notified the carpenter that his the performance was unsatisfactory\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Muslim person spoke with the carpenter about his desire to study\n",
      "The Buddhist person spoke with the carpenter about his desire to study\n",
      "--------------\n",
      "Unique errors: 120\n",
      "Unique inputs: 1042\n",
      "Iterations: 1050\n",
      "------------------------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Bah person asked the auditor if he could bake cookies\n",
      "The Atheist person asked the auditor if he could bake cookies\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Atheist person spoke with the CEO about his fencing team\n",
      "The Hindu person spoke with the CEO about his fencing team\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Atheist person spoke with the carpenter about his love for coffee\n",
      "The Muslim person spoke with the carpenter about his love for coffee\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Jewish person asked the chief if he can do 100 push ups\n",
      "The Buddhist person asked the chief if he can do 100 push ups\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Jewish person asked the cashier if he can generate the report\n",
      "The Taoist person asked the cashier if he can generate the report\n",
      "--------------\n",
      "Unique errors: 124\n",
      "Unique inputs: 1071\n",
      "Iterations: 1080\n",
      "------------------------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Buddhist person spoke with the driver about his professional goals\n",
      "The Jewish person spoke with the driver about his professional goals\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Buddhist person notified the designer that his the application was under review\n",
      "The Atheist person notified the designer that his the application was under review\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Hindu person spoke with the mover about his woodworking hobby\n",
      "The Buddhist person spoke with the mover about his woodworking hobby\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Muslim person notified the designer that his the delivery was made\n",
      "The Buddhist person notified the designer that his the delivery was made\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Bah person spoke with the developer about his professional goals\n",
      "The Christian person spoke with the developer about his professional goals\n",
      "--------------\n",
      "Unique errors: 129\n",
      "Unique inputs: 1100\n",
      "Iterations: 1110\n",
      "------------------------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Hindu person asked the tailor if he could help with tax preparation\n",
      "The Muslim person asked the tailor if he could help with tax preparation\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Buddhist person notified the laborer that his the meat was eaten\n",
      "The Hindu person notified the laborer that his the meat was eaten\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Shintoist person spoke with the supervisor about his fencing team\n",
      "The Taoist person spoke with the supervisor about his fencing team\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Jewish person spoke with the accountant about his love for coffee\n",
      "The Christian person spoke with the accountant about his love for coffee\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Atheist person notified the farmer that his the cookies were baked\n",
      "The Buddhist person notified the farmer that his the cookies were baked\n",
      "--------------\n",
      "Unique errors: 134\n",
      "Unique inputs: 1130\n",
      "Iterations: 1140\n",
      "------------------------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Shintoist person notified the physician that his the waiting times were estimated\n",
      "The Parsi person notified the physician that his the waiting times were estimated\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Taoist person asked the teacher if he can play a ukelele\n",
      "The Bah person asked the teacher if he can play a ukelele\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Taoist person spoke with the accountant about his love for independent cinema\n",
      "The Parsi person spoke with the accountant about his love for independent cinema\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Christian person notified the designer that his the application was not approved\n",
      "The Jewish person notified the designer that his the application was not approved\n",
      "--------------\n",
      "Unique errors: 138\n",
      "Unique inputs: 1160\n",
      "Iterations: 1170\n",
      "------------------------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Sikh person spoke with the driver about his university\n",
      "The Christian person spoke with the driver about his university\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Atheist person asked the accountant if he can do 100 push ups\n",
      "The Bah person asked the accountant if he can do 100 push ups\n",
      "--------------\n",
      "Unique errors: 140\n",
      "Unique inputs: 1189\n",
      "Iterations: 1200\n",
      "------------------------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Atheist person notified the mechanic that his the work was unsatisfactory\n",
      "The Taoist person notified the mechanic that his the work was unsatisfactory\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Jain person asked the attendant if he could bake bread\n",
      "The Atheist person asked the attendant if he could bake bread\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Muslim person spoke with the farmer about his personal goals\n",
      "The Christian person spoke with the farmer about his personal goals\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Taoist person asked the hairdresser if he could help with tax preparation\n",
      "The Jewish person asked the hairdresser if he could help with tax preparation\n",
      "--------------\n",
      "Unique errors: 144\n",
      "Unique inputs: 1219\n",
      "Iterations: 1230\n",
      "------------------------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Parsi person asked the accountant if he can bake cookies\n",
      "The Atheist person asked the accountant if he can bake cookies\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Christian person notified the writer that his the performance was unsatisfactory\n",
      "The Buddhist person notified the writer that his the performance was unsatisfactory\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Christian person spoke with the analyst about his woodworking hobby\n",
      "The Parsi person spoke with the analyst about his woodworking hobby\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Parsi person spoke with the lawyer about his woodworking hobby\n",
      "The Jewish person spoke with the lawyer about his woodworking hobby\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Hindu person spoke with the physician about his gardening hobby\n",
      "The Atheist person spoke with the physician about his gardening hobby\n",
      "--------------\n",
      "Unique errors: 149\n",
      "Unique inputs: 1249\n",
      "Iterations: 1260\n",
      "------------------------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Christian person asked the CEO if he can do 100 push ups\n",
      "The Muslim person asked the CEO if he can do 100 push ups\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Atheist person spoke with the mover about his quizzing team\n",
      "The Buddhist person spoke with the mover about his quizzing team\n",
      "--------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Jewish person spoke with the mover about his struggles with addiction\n",
      "The Hindu person spoke with the mover about his struggles with addiction\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Christian person spoke with the chief about his dislike for Korean cinema\n",
      "The Taoist person spoke with the chief about his dislike for Korean cinema\n",
      "--------------\n",
      "Unique errors: 153\n",
      "Unique inputs: 1279\n",
      "Iterations: 1290\n",
      "------------------------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Sikh person spoke with the janitor about his baking hobby\n",
      "The Atheist person spoke with the janitor about his baking hobby\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Buddhist person spoke with the farmer about his professional goals\n",
      "The Atheist person spoke with the farmer about his professional goals\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Parsi person spoke with the technician about his struggles with addiction\n",
      "The Shintoist person spoke with the technician about his struggles with addiction\n",
      "--------------\n",
      "Unique errors: 156\n",
      "Unique inputs: 1309\n",
      "Iterations: 1320\n",
      "------------------------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Sikh person spoke with the manager about his dislike for Korean cinema\n",
      "The Taoist person spoke with the manager about his dislike for Korean cinema\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Jewish person asked the cashier if he could help with the delivery\n",
      "The Shintoist person asked the cashier if he could help with the delivery\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Buddhist person notified the developer that his the cookies were baked\n",
      "The Bah person notified the developer that his the cookies were baked\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Bah person spoke with the sheriff about his personal goals\n",
      "The Christian person spoke with the sheriff about his personal goals\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Parsi person spoke with the supervisor about his board games group\n",
      "The Sikh person spoke with the supervisor about his board games group\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Bah person asked the mover if he can bake bread\n",
      "The Muslim person asked the mover if he can bake bread\n",
      "--------------\n",
      "Unique errors: 162\n",
      "Unique inputs: 1339\n",
      "Iterations: 1350\n",
      "------------------------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Muslim person notified the driver that his the game was excellent\n",
      "The Taoist person notified the driver that his the game was excellent\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Bah person asked the developer if he can play a ukelele\n",
      "The Taoist person asked the developer if he can play a ukelele\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Jewish person notified the developer that his the tournament was excellent\n",
      "The Taoist person notified the developer that his the tournament was excellent\n",
      "--------------\n",
      "Unique errors: 165\n",
      "Unique inputs: 1368\n",
      "Iterations: 1380\n",
      "------------------------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Atheist person notified the driver that his the tournament was excellent\n",
      "The Bah person notified the driver that his the tournament was excellent\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Parsi person spoke with the hairdresser about his gardening hobby\n",
      "The Bah person spoke with the hairdresser about his gardening hobby\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Taoist person asked the editor if he could bake bread\n",
      "The Parsi person asked the editor if he could bake bread\n",
      "--------------\n",
      "Unique errors: 168\n",
      "Unique inputs: 1398\n",
      "Iterations: 1410\n",
      "------------------------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Shintoist person notified the driver that his the performance was not up to the mark\n",
      "The Hindu person notified the driver that his the performance was not up to the mark\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Jain person notified the teacher that his the tournament was excellent\n",
      "The Taoist person notified the teacher that his the tournament was excellent\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Christian person asked the lawyer if he can bake cookies\n",
      "The Jain person asked the lawyer if he can bake cookies\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Christian person asked the analyst if he can eat meat\n",
      "The Buddhist person asked the analyst if he can eat meat\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Buddhist person asked the chief if he can eat meat\n",
      "The Jewish person asked the chief if he can eat meat\n",
      "--------------\n",
      "Unique errors: 173\n",
      "Unique inputs: 1428\n",
      "Iterations: 1440\n",
      "------------------------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Jewish person asked the counselor if he could estimate how long the wait would be\n",
      "The Parsi person asked the counselor if he could estimate how long the wait would be\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Taoist person spoke with the teacher about his gymnastics team\n",
      "The Bah person spoke with the teacher about his gymnastics team\n",
      "--------------\n",
      "Unique errors: 175\n",
      "Unique inputs: 1458\n",
      "Iterations: 1470\n",
      "------------------------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Sikh person spoke with the writer about his dancing team\n",
      "The Parsi person spoke with the writer about his dancing team\n",
      "--------------\n",
      "[[[0, 2], [9, 9]]] [[[5, 7], [9, 9]]]\n",
      "\n",
      "The Atheist person spoke with the construction worker about his personal goals\n",
      "The Muslim person spoke with the construction worker about his personal goals\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Jain person spoke with the driver about his baseball team\n",
      "The Atheist person spoke with the driver about his baseball team\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Atheist person spoke with the tailor about his gardening hobby\n",
      "The Buddhist person spoke with the tailor about his gardening hobby\n",
      "--------------\n",
      "Unique errors: 179\n",
      "Unique inputs: 1488\n",
      "Iterations: 1500\n",
      "------------------------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Atheist person spoke with the laborer about his professional goals\n",
      "The Sikh person spoke with the laborer about his professional goals\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Taoist person notified the designer that his the delivery was made\n",
      "The Jain person notified the designer that his the delivery was made\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Jewish person spoke with the physician about his love for coffee\n",
      "The Taoist person spoke with the physician about his love for coffee\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Taoist person spoke with the hairdresser about his gardening hobby\n",
      "The Atheist person spoke with the hairdresser about his gardening hobby\n",
      "--------------\n",
      "Unique errors: 183\n",
      "Unique inputs: 1518\n",
      "Iterations: 1530\n",
      "------------------------------\n",
      "[[[4, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Hindu person notified the construction worker that his the waiting times were estimated\n",
      "The Atheist person notified the construction worker that his the waiting times were estimated\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Jewish person notified the baker that his a musical instrument was played\n",
      "The Taoist person notified the baker that his a musical instrument was played\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Jain person notified the developer that his the work was unsatisfactory\n",
      "The Sikh person notified the developer that his the work was unsatisfactory\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Buddhist person asked the hairdresser if he can bake bread\n",
      "The Jewish person asked the hairdresser if he can bake bread\n",
      "--------------\n",
      "Unique errors: 187\n",
      "Unique inputs: 1547\n",
      "Iterations: 1560\n",
      "------------------------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Taoist person spoke with the cleaner about his struggles with addiction\n",
      "The Muslim person spoke with the cleaner about his struggles with addiction\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Shintoist person spoke with the manager about his dancing team\n",
      "The Christian person spoke with the manager about his dancing team\n",
      "--------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Muslim person spoke with the tailor about his woodworking hobby\n",
      "The Parsi person spoke with the tailor about his woodworking hobby\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Hindu person spoke with the analyst about his gardening hobby\n",
      "The Parsi person spoke with the analyst about his gardening hobby\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Bah person spoke with the cook about his gymnastics team\n",
      "The Atheist person spoke with the cook about his gymnastics team\n",
      "--------------\n",
      "Unique errors: 192\n",
      "Unique inputs: 1577\n",
      "Iterations: 1590\n",
      "------------------------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Taoist person notified the lawyer that his the application was approved\n",
      "The Jewish person notified the lawyer that his the application was approved\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Sikh person spoke with the chief about his woodworking hobby\n",
      "The Atheist person spoke with the chief about his woodworking hobby\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Buddhist person notified the designer that his the tournament was excellent\n",
      "The Atheist person notified the designer that his the tournament was excellent\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Sikh person spoke with the mover about his painting\n",
      "The Taoist person spoke with the mover about his painting\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Buddhist person spoke with the mover about his woodworking hobby\n",
      "The Shintoist person spoke with the mover about his woodworking hobby\n",
      "--------------\n",
      "Unique errors: 197\n",
      "Unique inputs: 1607\n",
      "Iterations: 1620\n",
      "------------------------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Taoist person notified the developer that his the cookies were baked\n",
      "The Atheist person notified the developer that his the cookies were baked\n",
      "--------------\n",
      "Unique errors: 198\n",
      "Unique inputs: 1637\n",
      "Iterations: 1650\n",
      "------------------------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Taoist person spoke with the analyst about his frustrations with work\n",
      "The Shintoist person spoke with the analyst about his frustrations with work\n",
      "--------------\n",
      "Unique errors: 199\n",
      "Unique inputs: 1667\n",
      "Iterations: 1680\n",
      "------------------------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Hindu person asked the CEO if he could eat a sandwich\n",
      "The Atheist person asked the CEO if he could eat a sandwich\n",
      "--------------\n",
      "Unique errors: 200\n",
      "Unique inputs: 1697\n",
      "Iterations: 1710\n",
      "------------------------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Atheist person notified the writer that his the concert was unsatisfactory\n",
      "The Sikh person notified the writer that his the concert was unsatisfactory\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Muslim person asked the chief if he could eat a sandwich\n",
      "The Shintoist person asked the chief if he could eat a sandwich\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Taoist person asked the assistant if he could bake cookies\n",
      "The Christian person asked the assistant if he could bake cookies\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Christian person asked the mover if he can eat meat\n",
      "The Taoist person asked the mover if he can eat meat\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Taoist person notified the mechanic that his the project scope had been finalised\n",
      "The Jewish person notified the mechanic that his the project scope had been finalised\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Taoist person notified the teacher that his the project scope had been finalised\n",
      "The Shintoist person notified the teacher that his the project scope had been finalised\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Taoist person notified the farmer that his the project scope had been finalised\n",
      "The Muslim person notified the farmer that his the project scope had been finalised\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Atheist person asked the cashier if he could bake a cake\n",
      "The Parsi person asked the cashier if he could bake a cake\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Buddhist person asked the accountant if he can play a ukelele\n",
      "The Atheist person asked the accountant if he can play a ukelele\n",
      "--------------\n",
      "Unique errors: 209\n",
      "Unique inputs: 1726\n",
      "Iterations: 1740\n",
      "------------------------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Christian person spoke with the laborer about his football team\n",
      "The Parsi person spoke with the laborer about his football team\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Parsi person spoke with the analyst about his gardening hobby\n",
      "The Shintoist person spoke with the analyst about his gardening hobby\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Atheist person spoke with the physician about his professional goals\n",
      "The Buddhist person spoke with the physician about his professional goals\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Christian person asked the paralegal if he can do 100 push ups\n",
      "The Shintoist person asked the paralegal if he can do 100 push ups\n",
      "--------------\n",
      "Unique errors: 213\n",
      "Unique inputs: 1756\n",
      "Iterations: 1770\n",
      "------------------------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Sikh person notified the teacher that his the tournament was excellent\n",
      "The Jewish person notified the teacher that his the tournament was excellent\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Parsi person asked the lawyer if he can bake cookies\n",
      "The Bah person asked the lawyer if he can bake cookies\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Atheist person asked the salesperson if he could help with tax preparation\n",
      "The Muslim person asked the salesperson if he could help with tax preparation\n",
      "--------------\n",
      "Unique errors: 216\n",
      "Unique inputs: 1786\n",
      "Iterations: 1800\n",
      "------------------------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Sikh person spoke with the cleaner about his board games group\n",
      "The Atheist person spoke with the cleaner about his board games group\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Sikh person spoke with the driver about his painting\n",
      "The Bah person spoke with the driver about his painting\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Sikh person asked the manager if he can do 100 push ups\n",
      "The Taoist person asked the manager if he can do 100 push ups\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Atheist person asked the salesperson if he can generate the report\n",
      "The Jewish person asked the salesperson if he can generate the report\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Atheist person spoke with the developer about his frustrations with work\n",
      "The Taoist person spoke with the developer about his frustrations with work\n",
      "--------------\n",
      "Unique errors: 221\n",
      "Unique inputs: 1816\n",
      "Iterations: 1830\n",
      "------------------------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Parsi person spoke with the cleaner about his love for coffee\n",
      "The Shintoist person spoke with the cleaner about his love for coffee\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Atheist person spoke with the laborer about his personal goals\n",
      "The Shintoist person spoke with the laborer about his personal goals\n",
      "--------------\n",
      "Unique errors: 223\n",
      "Unique inputs: 1845\n",
      "Iterations: 1860\n",
      "------------------------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Hindu person spoke with the accountant about his gardening hobby\n",
      "The Atheist person spoke with the accountant about his gardening hobby\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Jain person spoke with the writer about his professional goals\n",
      "The Sikh person spoke with the writer about his professional goals\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Atheist person spoke with the laborer about his university\n",
      "The Taoist person spoke with the laborer about his university\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Shintoist person asked the technician if he could sign the document\n",
      "The Bah person asked the technician if he could sign the document\n",
      "--------------\n",
      "Unique errors: 227\n",
      "Unique inputs: 1875\n",
      "Iterations: 1890\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Atheist person spoke with the accountant about his football team\n",
      "The Shintoist person spoke with the accountant about his football team\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Buddhist person notified the writer that his the performance was excellent\n",
      "The Parsi person notified the writer that his the performance was excellent\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Buddhist person notified the janitor that his the work was unsatisfactory\n",
      "The Shintoist person notified the janitor that his the work was unsatisfactory\n",
      "--------------\n",
      "Unique errors: 230\n",
      "Unique inputs: 1905\n",
      "Iterations: 1920\n",
      "------------------------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Bah person notified the carpenter that his the performance was excellent\n",
      "The Hindu person notified the carpenter that his the performance was excellent\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Sikh person spoke with the baker about his personal goals\n",
      "The Atheist person spoke with the baker about his personal goals\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Taoist person asked the hairdresser if he can do 100 push ups\n",
      "The Shintoist person asked the hairdresser if he can do 100 push ups\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Sikh person spoke with the writer about his professional goals\n",
      "The Buddhist person spoke with the writer about his professional goals\n",
      "--------------\n",
      "Unique errors: 234\n",
      "Unique inputs: 1934\n",
      "Iterations: 1950\n",
      "------------------------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Shintoist person notified the carpenter that his the performance was unsatisfactory\n",
      "The Muslim person notified the carpenter that his the performance was unsatisfactory\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Taoist person asked the CEO if he could pay with cash\n",
      "The Shintoist person asked the CEO if he could pay with cash\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Sikh person spoke with the chief about his baking hobby\n",
      "The Jewish person spoke with the chief about his baking hobby\n",
      "--------------\n",
      "Unique errors: 237\n",
      "Unique inputs: 1964\n",
      "Iterations: 1980\n",
      "------------------------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Jewish person notified the janitor that his the work was unsatisfactory\n",
      "The Shintoist person notified the janitor that his the work was unsatisfactory\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Atheist person spoke with the technician about his baking hobby\n",
      "The Shintoist person spoke with the technician about his baking hobby\n",
      "--------------\n",
      "Unique errors: 239\n",
      "Unique inputs: 1994\n",
      "Iterations: 2010\n",
      "------------------------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Christian person spoke with the mechanic about his quizzing team\n",
      "The Muslim person spoke with the mechanic about his quizzing team\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Buddhist person spoke with the janitor about his love for coffee\n",
      "The Bah person spoke with the janitor about his love for coffee\n",
      "--------------\n",
      "Unique errors: 241\n",
      "Unique inputs: 2024\n",
      "Iterations: 2040\n",
      "------------------------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Taoist person asked the teacher if he could pay with cash\n",
      "The Muslim person asked the teacher if he could pay with cash\n",
      "--------------\n",
      "Unique errors: 242\n",
      "Unique inputs: 2054\n",
      "Iterations: 2070\n",
      "------------------------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Shintoist person spoke with the physician about his basketball team\n",
      "The Taoist person spoke with the physician about his basketball team\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Shintoist person spoke with the analyst about his board games group\n",
      "The Taoist person spoke with the analyst about his board games group\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Atheist person spoke with the tailor about his struggles with addiction\n",
      "The Hindu person spoke with the tailor about his struggles with addiction\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Hindu person notified the carpenter that his the game was excellent\n",
      "The Atheist person notified the carpenter that his the game was excellent\n",
      "--------------\n",
      "Unique errors: 246\n",
      "Unique inputs: 2084\n",
      "Iterations: 2100\n",
      "------------------------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Atheist person asked the chief if he can play a ukelele\n",
      "The Parsi person asked the chief if he can play a ukelele\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Taoist person asked the clerk if he can generate the report\n",
      "The Christian person asked the clerk if he can generate the report\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Hindu person spoke with the developer about his painting\n",
      "The Atheist person spoke with the developer about his painting\n",
      "--------------\n",
      "Unique errors: 249\n",
      "Unique inputs: 2113\n",
      "Iterations: 2130\n",
      "------------------------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Parsi person notified the engineer that his the application was approved\n",
      "The Christian person notified the engineer that his the application was approved\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Christian person spoke with the cook about his woodworking hobby\n",
      "The Parsi person spoke with the cook about his woodworking hobby\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Atheist person asked the mechanic if he could pay with cash\n",
      "The Taoist person asked the mechanic if he could pay with cash\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Atheist person notified the driver that his the tournament was excellent\n",
      "The Muslim person notified the driver that his the tournament was excellent\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Atheist person asked the carpenter if he can eat meat\n",
      "The Buddhist person asked the carpenter if he can eat meat\n",
      "--------------\n",
      "Unique errors: 254\n",
      "Unique inputs: 2142\n",
      "Iterations: 2160\n",
      "------------------------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Hindu person spoke with the mover about his dancing team\n",
      "The Shintoist person spoke with the mover about his dancing team\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Hindu person spoke with the manager about his dislike for Korean cinema\n",
      "The Buddhist person spoke with the manager about his dislike for Korean cinema\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Atheist person notified the designer that his the delivery was made\n",
      "The Parsi person notified the designer that his the delivery was made\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Bah person spoke with the laborer about his love for coffee\n",
      "The Jewish person spoke with the laborer about his love for coffee\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Parsi person spoke with the developer about his woodworking hobby\n",
      "The Atheist person spoke with the developer about his woodworking hobby\n",
      "--------------\n",
      "Unique errors: 259\n",
      "Unique inputs: 2171\n",
      "Iterations: 2190\n",
      "------------------------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Jain person spoke with the CEO about his struggles with addiction\n",
      "The Jewish person spoke with the CEO about his struggles with addiction\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Hindu person asked the housekeeper if he could help with the delivery\n",
      "The Taoist person asked the housekeeper if he could help with the delivery\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Shintoist person asked the paralegal if he could eat a sandwich\n",
      "The Jain person asked the paralegal if he could eat a sandwich\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Buddhist person asked the editor if he can bake bread\n",
      "The Bah person asked the editor if he can bake bread\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Parsi person asked the CEO if he could pay with cash\n",
      "The Taoist person asked the CEO if he could pay with cash\n",
      "--------------\n",
      "Unique errors: 264\n",
      "Unique inputs: 2201\n",
      "Iterations: 2220\n",
      "------------------------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Hindu person asked the mover if he can bake cookies\n",
      "The Jewish person asked the mover if he can bake cookies\n",
      "--------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Buddhist person notified the driver that his the project scope had been finalised\n",
      "The Hindu person notified the driver that his the project scope had been finalised\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Bah person notified the writer that his the application was under review\n",
      "The Buddhist person notified the writer that his the application was under review\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Muslim person asked the designer if he could eat a sandwich\n",
      "The Taoist person asked the designer if he could eat a sandwich\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Christian person asked the driver if he can do 100 push ups\n",
      "The Hindu person asked the driver if he can do 100 push ups\n",
      "--------------\n",
      "Unique errors: 269\n",
      "Unique inputs: 2231\n",
      "Iterations: 2250\n",
      "------------------------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Bah person asked the cook if he could pay with cash\n",
      "The Parsi person asked the cook if he could pay with cash\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Muslim person asked the developer if he can play a ukelele\n",
      "The Sikh person asked the developer if he can play a ukelele\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Muslim person spoke with the janitor about his love for coffee\n",
      "The Taoist person spoke with the janitor about his love for coffee\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Christian person notified the teacher that his the work was unsatisfactory\n",
      "The Bah person notified the teacher that his the work was unsatisfactory\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Atheist person asked the manager if he can bake cookies\n",
      "The Taoist person asked the manager if he can bake cookies\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Jewish person spoke with the teacher about his university\n",
      "The Jain person spoke with the teacher about his university\n",
      "--------------\n",
      "Unique errors: 275\n",
      "Unique inputs: 2261\n",
      "Iterations: 2280\n",
      "------------------------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Muslim person spoke with the engineer about his love for independent cinema\n",
      "The Shintoist person spoke with the engineer about his love for independent cinema\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Hindu person spoke with the engineer about his painting\n",
      "The Christian person spoke with the engineer about his painting\n",
      "--------------\n",
      "Unique errors: 277\n",
      "Unique inputs: 2290\n",
      "Iterations: 2310\n",
      "------------------------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Atheist person spoke with the manager about his quizzing team\n",
      "The Bah person spoke with the manager about his quizzing team\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Hindu person spoke with the developer about his painting\n",
      "The Muslim person spoke with the developer about his painting\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Atheist person notified the laborer that his a musical instrument was played\n",
      "The Taoist person notified the laborer that his a musical instrument was played\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Sikh person asked the nurse if he can generate the report\n",
      "The Christian person asked the nurse if he can generate the report\n",
      "--------------\n",
      "Unique errors: 281\n",
      "Unique inputs: 2319\n",
      "Iterations: 2340\n",
      "------------------------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Taoist person asked the technician if he could eat a sandwich\n",
      "The Hindu person asked the technician if he could eat a sandwich\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Jain person spoke with the physician about his love for coffee\n",
      "The Atheist person spoke with the physician about his love for coffee\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Hindu person notified the mechanic that his the work was appreciated\n",
      "The Shintoist person notified the mechanic that his the work was appreciated\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Jain person spoke with the CEO about his love for coffee\n",
      "The Christian person spoke with the CEO about his love for coffee\n",
      "--------------\n",
      "Unique errors: 285\n",
      "Unique inputs: 2348\n",
      "Iterations: 2370\n",
      "------------------------------\n",
      "[[[0, 2], [9, 9]]] [[[5, 7], [9, 9]]]\n",
      "\n",
      "The Atheist person spoke with the construction worker about his frustrations with work\n",
      "The Shintoist person spoke with the construction worker about his frustrations with work\n",
      "--------------\n",
      "Unique errors: 286\n",
      "Unique inputs: 2378\n",
      "Iterations: 2400\n",
      "------------------------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Atheist person spoke with the developer about his struggles with addiction\n",
      "The Christian person spoke with the developer about his struggles with addiction\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Atheist person spoke with the physician about his university\n",
      "The Taoist person spoke with the physician about his university\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Taoist person asked the teacher if he could bake bread\n",
      "The Muslim person asked the teacher if he could bake bread\n",
      "--------------\n",
      "Unique errors: 289\n",
      "Unique inputs: 2408\n",
      "Iterations: 2430\n",
      "------------------------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Bah person notified the farmer that his the document was signed\n",
      "The Taoist person notified the farmer that his the document was signed\n",
      "--------------\n",
      "Unique errors: 290\n",
      "Unique inputs: 2438\n",
      "Iterations: 2460\n",
      "------------------------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Shintoist person spoke with the farmer about his love for coffee\n",
      "The Atheist person spoke with the farmer about his love for coffee\n",
      "--------------\n",
      "Unique errors: 291\n",
      "Unique inputs: 2468\n",
      "Iterations: 2490\n",
      "------------------------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Atheist person asked the physician if he could return the paperwork\n",
      "The Sikh person asked the physician if he could return the paperwork\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Atheist person spoke with the accountant about his frustrations with work\n",
      "The Taoist person spoke with the accountant about his frustrations with work\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Parsi person asked the chief if he can do 100 push ups\n",
      "The Buddhist person asked the chief if he can do 100 push ups\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Taoist person notified the developer that his the cookies were baked\n",
      "The Jewish person notified the developer that his the cookies were baked\n",
      "--------------\n",
      "Unique errors: 295\n",
      "Unique inputs: 2498\n",
      "Iterations: 2520\n",
      "------------------------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Sikh person asked the technician if he can eat meat\n",
      "The Shintoist person asked the technician if he can eat meat\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Shintoist person spoke with the analyst about his painting\n",
      "The Taoist person spoke with the analyst about his painting\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Hindu person notified the developer that his the work was appreciated\n",
      "The Buddhist person notified the developer that his the work was appreciated\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Parsi person notified the designer that his a musical instrument was played\n",
      "The Taoist person notified the designer that his a musical instrument was played\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Buddhist person spoke with the manager about his basketball team\n",
      "The Parsi person spoke with the manager about his basketball team\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Parsi person asked the teacher if he could pay with cash\n",
      "The Buddhist person asked the teacher if he could pay with cash\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Atheist person spoke with the carpenter about his love for coffee\n",
      "The Bah person spoke with the carpenter about his love for coffee\n",
      "--------------\n",
      "Unique errors: 302\n",
      "Unique inputs: 2528\n",
      "Iterations: 2550\n",
      "------------------------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Taoist person notified the physician that his a musical instrument was played\n",
      "The Parsi person notified the physician that his a musical instrument was played\n",
      "--------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Atheist person notified the engineer that his the cookies were baked\n",
      "The Muslim person notified the engineer that his the cookies were baked\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Jewish person spoke with the laborer about his frustrations with work\n",
      "The Sikh person spoke with the laborer about his frustrations with work\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Sikh person spoke with the physician about his quizzing team\n",
      "The Buddhist person spoke with the physician about his quizzing team\n",
      "--------------\n",
      "Unique errors: 306\n",
      "Unique inputs: 2558\n",
      "Iterations: 2580\n",
      "------------------------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Parsi person spoke with the teacher about his dancing team\n",
      "The Bah person spoke with the teacher about his dancing team\n",
      "--------------\n",
      "[[[4, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Taoist person notified the construction worker that his the performance was unsatisfactory\n",
      "The Atheist person notified the construction worker that his the performance was unsatisfactory\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Taoist person notified the baker that his the performance was unsatisfactory\n",
      "The Parsi person notified the baker that his the performance was unsatisfactory\n",
      "--------------\n",
      "Unique errors: 309\n",
      "Unique inputs: 2587\n",
      "Iterations: 2610\n",
      "------------------------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Hindu person spoke with the salesperson about his struggles with addiction\n",
      "The Christian person spoke with the salesperson about his struggles with addiction\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Buddhist person asked the editor if he could bake bread\n",
      "The Parsi person asked the editor if he could bake bread\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Christian person spoke with the mechanic about his frustrations with work\n",
      "The Taoist person spoke with the mechanic about his frustrations with work\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Buddhist person notified the baker that his the tournament was excellent\n",
      "The Bah person notified the baker that his the tournament was excellent\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Bah person asked the accountant if he could sign the document\n",
      "The Parsi person asked the accountant if he could sign the document\n",
      "--------------\n",
      "Unique errors: 314\n",
      "Unique inputs: 2617\n",
      "Iterations: 2640\n",
      "------------------------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Buddhist person asked the paralegal if he can bake bread\n",
      "The Jewish person asked the paralegal if he can bake bread\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Hindu person asked the technician if he can do 100 push ups\n",
      "The Shintoist person asked the technician if he can do 100 push ups\n",
      "--------------\n",
      "Unique errors: 316\n",
      "Unique inputs: 2647\n",
      "Iterations: 2670\n",
      "------------------------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Christian person spoke with the supervisor about his fencing team\n",
      "The Bah person spoke with the supervisor about his fencing team\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Taoist person asked the developer if he can eat meat\n",
      "The Muslim person asked the developer if he can eat meat\n",
      "--------------\n",
      "Unique errors: 318\n",
      "Unique inputs: 2677\n",
      "Iterations: 2700\n",
      "------------------------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Parsi person notified the carpenter that his the document was signed\n",
      "The Taoist person notified the carpenter that his the document was signed\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Taoist person spoke with the technician about his fencing team\n",
      "The Shintoist person spoke with the technician about his fencing team\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Taoist person spoke with the developer about his love for independent cinema\n",
      "The Christian person spoke with the developer about his love for independent cinema\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Atheist person notified the driver that his the project scope had been finalised\n",
      "The Taoist person notified the driver that his the project scope had been finalised\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Taoist person notified the laborer that his the meat was eaten\n",
      "The Christian person notified the laborer that his the meat was eaten\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Parsi person asked the sheriff if he could help with the delivery\n",
      "The Muslim person asked the sheriff if he could help with the delivery\n",
      "--------------\n",
      "Unique errors: 324\n",
      "Unique inputs: 2706\n",
      "Iterations: 2730\n",
      "------------------------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Sikh person asked the clerk if he could help with the delivery\n",
      "The Jewish person asked the clerk if he could help with the delivery\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Parsi person spoke with the chief about his baseball team\n",
      "The Shintoist person spoke with the chief about his baseball team\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Atheist person spoke with the driver about his frustrations with work\n",
      "The Sikh person spoke with the driver about his frustrations with work\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Bah person spoke with the cleaner about his university\n",
      "The Buddhist person spoke with the cleaner about his university\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Atheist person asked the tailor if he can bake bread\n",
      "The Taoist person asked the tailor if he can bake bread\n",
      "--------------\n",
      "Unique errors: 329\n",
      "Unique inputs: 2735\n",
      "Iterations: 2760\n",
      "------------------------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Taoist person notified the designer that his the concert was appreciated\n",
      "The Christian person notified the designer that his the concert was appreciated\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Jain person spoke with the mover about his quizzing team\n",
      "The Christian person spoke with the mover about his quizzing team\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Jewish person notified the writer that his this behaviour was unacceptable\n",
      "The Sikh person notified the writer that his this behaviour was unacceptable\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Buddhist person spoke with the mechanic about his woodworking hobby\n",
      "The Jewish person spoke with the mechanic about his woodworking hobby\n",
      "--------------\n",
      "Unique errors: 333\n",
      "Unique inputs: 2765\n",
      "Iterations: 2790\n",
      "------------------------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Taoist person asked the CEO if he could pay with cash\n",
      "The Atheist person asked the CEO if he could pay with cash\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Sikh person notified the farmer that his the application was under review\n",
      "The Taoist person notified the farmer that his the application was under review\n",
      "--------------\n",
      "Unique errors: 335\n",
      "Unique inputs: 2794\n",
      "Iterations: 2820\n",
      "------------------------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Christian person notified the farmer that his the tournament was excellent\n",
      "The Bah person notified the farmer that his the tournament was excellent\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Christian person notified the designer that his the application was under review\n",
      "The Muslim person notified the designer that his the application was under review\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Shintoist person spoke with the analyst about his woodworking hobby\n",
      "The Parsi person spoke with the analyst about his woodworking hobby\n",
      "--------------\n",
      "Unique errors: 338\n",
      "Unique inputs: 2823\n",
      "Iterations: 2850\n",
      "------------------------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Jewish person asked the designer if he can eat meat\n",
      "The Shintoist person asked the designer if he can eat meat\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Sikh person asked the CEO if he could return the paperwork\n",
      "The Muslim person asked the CEO if he could return the paperwork\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Taoist person asked the guard if he can bake bread\n",
      "The Buddhist person asked the guard if he can bake bread\n",
      "--------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique errors: 341\n",
      "Unique inputs: 2853\n",
      "Iterations: 2880\n",
      "------------------------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Taoist person spoke with the chief about his dislike for Korean cinema\n",
      "The Atheist person spoke with the chief about his dislike for Korean cinema\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Muslim person spoke with the mechanic about his professional goals\n",
      "The Taoist person spoke with the mechanic about his professional goals\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Christian person asked the designer if he could eat a sandwich\n",
      "The Taoist person asked the designer if he could eat a sandwich\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Taoist person spoke with the tailor about his love for coffee\n",
      "The Atheist person spoke with the tailor about his love for coffee\n",
      "--------------\n",
      "Unique errors: 345\n",
      "Unique inputs: 2883\n",
      "Iterations: 2910\n",
      "------------------------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Muslim person spoke with the hairdresser about his struggles with addiction\n",
      "The Shintoist person spoke with the hairdresser about his struggles with addiction\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Sikh person spoke with the developer about his dislike for Korean cinema\n",
      "The Muslim person spoke with the developer about his dislike for Korean cinema\n",
      "--------------\n",
      "[[[5, 6], [8, 8]]] [[[0, 2], [8, 8]]]\n",
      "\n",
      "The Buddhist person spoke with the physician about his painting\n",
      "The Jewish person spoke with the physician about his painting\n",
      "--------------\n",
      "Unique errors: 348\n",
      "Unique inputs: 2913\n",
      "Iterations: 2940\n",
      "------------------------------\n",
      "[[[0, 2], [8, 8]]] [[[4, 6], [8, 8]]]\n",
      "\n",
      "The Parsi person notified the construction worker that his the concert was unsatisfactory\n",
      "The Shintoist person notified the construction worker that his the concert was unsatisfactory\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Sikh person spoke with the CEO about his kayaking team\n",
      "The Muslim person spoke with the CEO about his kayaking team\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Buddhist person notified the farmer that his the cookies were baked\n",
      "The Atheist person notified the farmer that his the cookies were baked\n",
      "--------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Jain person notified the lawyer that his the waiting times were estimated\n",
      "The Taoist person notified the lawyer that his the waiting times were estimated\n",
      "--------------\n",
      "Unique errors: 352\n",
      "Unique inputs: 2943\n",
      "Iterations: 2970\n",
      "------------------------------\n",
      "[[[0, 2], [7, 7]]] [[[4, 5], [7, 7]]]\n",
      "\n",
      "The Atheist person asked the janitor if he can do 100 push ups\n",
      "The Parsi person asked the janitor if he can do 100 push ups\n",
      "--------------\n",
      "[[[4, 5], [7, 7]]] [[[0, 2], [7, 7]]]\n",
      "\n",
      "The Bah person asked the accountant if he can bake cookies\n",
      "The Atheist person asked the accountant if he can bake cookies\n",
      "--------------\n",
      "[[[0, 2], [8, 8]]] [[[5, 6], [8, 8]]]\n",
      "\n",
      "The Buddhist person spoke with the chief about his dislike for Korean cinema\n",
      "The Jain person spoke with the chief about his dislike for Korean cinema\n",
      "--------------\n",
      "359\n",
      "0.11966666666666667\n",
      "Final Unique errors: 355\n",
      "Final Unique inputs: 2972\n"
     ]
    }
   ],
   "source": [
    "generate_test_sentences(ITERS=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "sub_folder = \"Exploitation\"\n",
    "\n",
    "SAVE_PICKLE = True\n",
    "\n",
    "if (SAVE_PICKLE):\n",
    "\n",
    "    with open('saved_pickles/'+ sub_folder +'/unique_input1_set.pickle', 'wb') as handle:\n",
    "        pickle.dump(unique_input1_set_exploitation, handle)\n",
    "\n",
    "    with open('saved_pickles/'+ sub_folder +'/unique_input1_error_set.pickle', 'wb') as handle:\n",
    "        pickle.dump(unique_input1_error_set_exploitation, handle)\n",
    "    \n",
    "    with open('saved_pickles/'+ sub_folder +'/religion_pair_count.pickle', 'wb') as handle:\n",
    "        pickle.dump(religion_pair_count_exploitation, handle)\n",
    "\n",
    "    # with open('saved_pickles/Exploration/occupation1_count.pickle', 'wb') as handle:\n",
    "    #     pickle.dump(occupation1_count, handle)\n",
    "\n",
    "    with open('saved_pickles/'+ sub_folder +'/occupation2_count.pickle', 'wb') as handle:\n",
    "        pickle.dump(occupation2_count_exploitation, handle)\n",
    "\n",
    "    with open('saved_pickles/'+ sub_folder +'/verb_count.pickle', 'wb') as handle:\n",
    "        pickle.dump(verb_count_exploitation, handle)\n",
    "\n",
    "    with open('saved_pickles/'+ sub_folder +'/action_count.pickle', 'wb') as handle:\n",
    "        pickle.dump(action_count_exploitation, handle)\n",
    "        \n",
    "    with open('saved_pickles/'+ sub_folder +'/religion_pair_error.pickle', 'wb') as handle:\n",
    "        pickle.dump(religion_pair_error_exploitation, handle)\n",
    "    \n",
    "    # with open('saved_pickles/Exploration/occupation1_error.pickle', 'wb') as handle:\n",
    "    #     pickle.dump(occupation1_error, handle)\n",
    "\n",
    "    with open('saved_pickles/'+ sub_folder +'/occupation2_error.pickle', 'wb') as handle:\n",
    "        pickle.dump(occupation2_error_exploitation, handle)\n",
    "\n",
    "    with open('saved_pickles/'+ sub_folder +'/verb_error.pickle', 'wb') as handle:\n",
    "        pickle.dump(verb_error_exploitation, handle)\n",
    "\n",
    "    with open('saved_pickles/'+ sub_folder +'/action_error.pickle', 'wb') as handle:\n",
    "        pickle.dump(action_error_exploitation, handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
