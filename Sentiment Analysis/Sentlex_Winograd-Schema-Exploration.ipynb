{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentlex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentlex\n",
    "# import sentlex.sentanalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text='''we had a great time at the tirreno hotel, very friendly and helpful, nothing was ever too much trouble. \n",
    "the rooms were in excellent condition, very clean and comfortable.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sentlex' has no attribute 'SWN3Lexicon'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-8edd0f024132>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mSWN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentlex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSWN3Lexicon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'sentlex' has no attribute 'SWN3Lexicon'"
     ]
    }
   ],
   "source": [
    "SWN = sentlex.SWN3Lexicon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"VADER is smart, handsome, and funny.\",  # positive sentence example\n",
    "             \"VADER is smart, handsome, and funny!\",  # punctuation emphasis handled correctly (sentiment intensity adjusted)\n",
    "             \"VADER is very smart, handsome, and funny.\", # booster words handled correctly (sentiment intensity adjusted)\n",
    "             \"VADER is VERY SMART, handsome, and FUNNY.\",  # emphasis for ALLCAPS handled\n",
    "             \"VADER is VERY SMART, handsome, and FUNNY!!!\", # combination of signals - VADER appropriately adjusts intensity\n",
    "             \"VADER is VERY SMART, uber handsome, and FRIGGIN FUNNY!!!\", # booster words & punctuation make this close to ceiling for score\n",
    "             \"VADER is not smart, handsome, nor funny.\",  # negation sentence example\n",
    "             \"The book was good.\",  # positive sentence\n",
    "             \"At least it isn't a horrible book.\",  # negated negative sentence with contraction\n",
    "             \"The book was only kind of good.\", # qualified positive sentence is handled correctly (intensity adjusted)\n",
    "             \"The plot was good, but the characters are uncompelling and the dialog is not great.\", # mixed negation sentence\n",
    "             \"Today SUX!\",  # negative slang with capitalization emphasis\n",
    "             \"Today only kinda sux! But I'll get by, lol\", # mixed sentiment example with slang and constrastive conjunction \"but\"\n",
    "             \"Make sure you :) or :D today!\",  # emoticons handled\n",
    "             \"Catch utf-8 emoji such as such as 游눚 and 游눎 and 游때\",  # emojis handled\n",
    "             \"Not bad at all\"  # Capitalized negation\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER is smart, handsome, and funny.----------------------------- {'neg': 0.0, 'neu': 0.254, 'pos': 0.746, 'compound': 0.8316}\n",
      "VADER is smart, handsome, and funny!----------------------------- {'neg': 0.0, 'neu': 0.248, 'pos': 0.752, 'compound': 0.8439}\n",
      "VADER is very smart, handsome, and funny.------------------------ {'neg': 0.0, 'neu': 0.299, 'pos': 0.701, 'compound': 0.8545}\n",
      "VADER is VERY SMART, handsome, and FUNNY.------------------------ {'neg': 0.0, 'neu': 0.246, 'pos': 0.754, 'compound': 0.9227}\n",
      "VADER is VERY SMART, handsome, and FUNNY!!!---------------------- {'neg': 0.0, 'neu': 0.233, 'pos': 0.767, 'compound': 0.9342}\n",
      "VADER is VERY SMART, uber handsome, and FRIGGIN FUNNY!!!--------- {'neg': 0.0, 'neu': 0.294, 'pos': 0.706, 'compound': 0.9469}\n",
      "VADER is not smart, handsome, nor funny.------------------------- {'neg': 0.646, 'neu': 0.354, 'pos': 0.0, 'compound': -0.7424}\n",
      "The book was good.----------------------------------------------- {'neg': 0.0, 'neu': 0.508, 'pos': 0.492, 'compound': 0.4404}\n",
      "At least it isn't a horrible book.------------------------------- {'neg': 0.0, 'neu': 0.678, 'pos': 0.322, 'compound': 0.431}\n",
      "The book was only kind of good.---------------------------------- {'neg': 0.0, 'neu': 0.697, 'pos': 0.303, 'compound': 0.3832}\n",
      "The plot was good, but the characters are uncompelling and the dialog is not great. {'neg': 0.327, 'neu': 0.579, 'pos': 0.094, 'compound': -0.7042}\n",
      "Today SUX!------------------------------------------------------- {'neg': 0.779, 'neu': 0.221, 'pos': 0.0, 'compound': -0.5461}\n",
      "Today only kinda sux! But I'll get by, lol----------------------- {'neg': 0.127, 'neu': 0.556, 'pos': 0.317, 'compound': 0.5249}\n",
      "Make sure you :) or :D today!------------------------------------ {'neg': 0.0, 'neu': 0.294, 'pos': 0.706, 'compound': 0.8633}\n",
      "Catch utf-8 emoji such as such as 游눚 and 游눎 and 游때------------------ {'neg': 0.0, 'neu': 0.615, 'pos': 0.385, 'compound': 0.875}\n",
      "Not bad at all--------------------------------------------------- {'neg': 0.0, 'neu': 0.513, 'pos': 0.487, 'compound': 0.431}\n"
     ]
    }
   ],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "for sentence in sentences:\n",
    "    vs = analyzer.polarity_scores(sentence)\n",
    "    print(\"{:-<65} {}\".format(sentence, str(vs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"He is smart, handsome, and funny.\",  # positive sentence example\n",
    "             \"He is smart, handsome, and funny!\",  # punctuation emphasis handled correctly (sentiment intensity adjusted)\n",
    "             \"He is very smart, handsome, and funny.\", # booster words handled correctly (sentiment intensity adjusted)\n",
    "             \"He is VERY SMART, handsome, and FUNNY.\",  # emphasis for ALLCAPS handled\n",
    "             \"He is VERY SMART, handsome, and FUNNY!!!\", # combination of signals - He appropriately adjusts intensity\n",
    "             \"He is VERY SMART, uber handsome, and FRIGGIN FUNNY!!!\", # booster words & punctuation make this close to ceiling for score\n",
    "             \"He is not smart, handsome, nor funny.\",  # negation sentence example\n",
    "             \"The book was good.\",  # positive sentence\n",
    "             \"At least it isn't a horrible book.\",  # negated negative sentence with contraction\n",
    "             \"The book was only kind of good.\", # qualified positive sentence is handled correctly (intensity adjusted)\n",
    "             \"The plot was good, but the characters are uncompelling and the dialog is not great.\", # mixed negation sentence\n",
    "             \"Today SUX!\",  # negative slang with capitalization emphasis\n",
    "             \"Today only kinda sux! But I'll get by, lol\", # mixed sentiment example with slang and constrastive conjunction \"but\"\n",
    "             \"Make sure you :) or :D today!\",  # emoticons handled\n",
    "             \"Catch utf-8 emoji such as such as 游눚 and 游눎 and 游때\",  # emojis handled\n",
    "             \"Not bad at all\"  # Capitalized negation\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He is smart, handsome, and funny.-------------------------------- {'neg': 0.0, 'neu': 0.254, 'pos': 0.746, 'compound': 0.8316}\n",
      "He is smart, handsome, and funny!-------------------------------- {'neg': 0.0, 'neu': 0.248, 'pos': 0.752, 'compound': 0.8439}\n",
      "He is very smart, handsome, and funny.--------------------------- {'neg': 0.0, 'neu': 0.299, 'pos': 0.701, 'compound': 0.8545}\n",
      "He is VERY SMART, handsome, and FUNNY.--------------------------- {'neg': 0.0, 'neu': 0.246, 'pos': 0.754, 'compound': 0.9227}\n",
      "He is VERY SMART, handsome, and FUNNY!!!------------------------- {'neg': 0.0, 'neu': 0.233, 'pos': 0.767, 'compound': 0.9342}\n",
      "He is VERY SMART, uber handsome, and FRIGGIN FUNNY!!!------------ {'neg': 0.0, 'neu': 0.294, 'pos': 0.706, 'compound': 0.9469}\n",
      "He is not smart, handsome, nor funny.---------------------------- {'neg': 0.646, 'neu': 0.354, 'pos': 0.0, 'compound': -0.7424}\n",
      "The book was good.----------------------------------------------- {'neg': 0.0, 'neu': 0.508, 'pos': 0.492, 'compound': 0.4404}\n",
      "At least it isn't a horrible book.------------------------------- {'neg': 0.0, 'neu': 0.678, 'pos': 0.322, 'compound': 0.431}\n",
      "The book was only kind of good.---------------------------------- {'neg': 0.0, 'neu': 0.697, 'pos': 0.303, 'compound': 0.3832}\n",
      "The plot was good, but the characters are uncompelling and the dialog is not great. {'neg': 0.327, 'neu': 0.579, 'pos': 0.094, 'compound': -0.7042}\n",
      "Today SUX!------------------------------------------------------- {'neg': 0.779, 'neu': 0.221, 'pos': 0.0, 'compound': -0.5461}\n",
      "Today only kinda sux! But I'll get by, lol----------------------- {'neg': 0.127, 'neu': 0.556, 'pos': 0.317, 'compound': 0.5249}\n",
      "Make sure you :) or :D today!------------------------------------ {'neg': 0.0, 'neu': 0.294, 'pos': 0.706, 'compound': 0.8633}\n",
      "Catch utf-8 emoji such as such as 游눚 and 游눎 and 游때------------------ {'neg': 0.0, 'neu': 0.615, 'pos': 0.385, 'compound': 0.875}\n",
      "Not bad at all--------------------------------------------------- {'neg': 0.0, 'neu': 0.513, 'pos': 0.487, 'compound': 0.431}\n"
     ]
    }
   ],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "for sentence in sentences:\n",
    "    vs = analyzer.polarity_scores(sentence)\n",
    "    print(\"{:-<65} {}\".format(sentence, str(vs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"She is smart, handsome, and funny.\",  # positive sentence example\n",
    "             \"She is smart, handsome, and funny!\",  # punctuation emphasis handled correctly (sentiment intensity adjusted)\n",
    "             \"She is very smart, handsome, and funny.\", # booster words handled correctly (sentiment intensity adjusted)\n",
    "             \"She is VERY SMART, handsome, and FUNNY.\",  # emphasis for ALLCAPS handled\n",
    "             \"She is VERY SMART, handsome, and FUNNY!!!\", # combination of signals - She appropriately adjusts intensity\n",
    "             \"She is VERY SMART, uber handsome, and FRIGGIN FUNNY!!!\", # booster words & punctuation make this close to ceiling for score\n",
    "             \"She is not smart, handsome, nor funny.\",  # negation sentence example\n",
    "             \"TShe book was good.\",  # positive sentence\n",
    "             \"At least it isn't a horrible book.\",  # negated negative sentence with contraction\n",
    "             \"TShe book was only kind of good.\", # qualified positive sentence is handled correctly (intensity adjusted)\n",
    "             \"TShe plot was good, but tShe characters are uncompelling and tShe dialog is not great.\", # mixed negation sentence\n",
    "             \"Today SUX!\",  # negative slang with capitalization emphasis\n",
    "             \"Today only kinda sux! But I'll get by, lol\", # mixed sentiment example with slang and constrastive conjunction \"but\"\n",
    "             \"Make sure you :) or :D today!\",  # emoticons handled\n",
    "             \"Catch utf-8 emoji such as such as 游눚 and 游눎 and 游때\",  # emojis handled\n",
    "             \"Not bad at all\"  # Capitalized negation\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She is smart, handsome, and funny.------------------------------- {'neg': 0.0, 'neu': 0.254, 'pos': 0.746, 'compound': 0.8316}\n",
      "She is smart, handsome, and funny!------------------------------- {'neg': 0.0, 'neu': 0.248, 'pos': 0.752, 'compound': 0.8439}\n",
      "She is very smart, handsome, and funny.-------------------------- {'neg': 0.0, 'neu': 0.299, 'pos': 0.701, 'compound': 0.8545}\n",
      "She is VERY SMART, handsome, and FUNNY.-------------------------- {'neg': 0.0, 'neu': 0.246, 'pos': 0.754, 'compound': 0.9227}\n",
      "She is VERY SMART, handsome, and FUNNY!!!------------------------ {'neg': 0.0, 'neu': 0.233, 'pos': 0.767, 'compound': 0.9342}\n",
      "She is VERY SMART, uber handsome, and FRIGGIN FUNNY!!!----------- {'neg': 0.0, 'neu': 0.294, 'pos': 0.706, 'compound': 0.9469}\n",
      "She is not smart, handsome, nor funny.--------------------------- {'neg': 0.646, 'neu': 0.354, 'pos': 0.0, 'compound': -0.7424}\n",
      "TShe book was good.---------------------------------------------- {'neg': 0.0, 'neu': 0.508, 'pos': 0.492, 'compound': 0.4404}\n",
      "At least it isn't a horrible book.------------------------------- {'neg': 0.0, 'neu': 0.678, 'pos': 0.322, 'compound': 0.431}\n",
      "TShe book was only kind of good.--------------------------------- {'neg': 0.0, 'neu': 0.697, 'pos': 0.303, 'compound': 0.3832}\n",
      "TShe plot was good, but tShe characters are uncompelling and tShe dialog is not great. {'neg': 0.327, 'neu': 0.579, 'pos': 0.094, 'compound': -0.7042}\n",
      "Today SUX!------------------------------------------------------- {'neg': 0.779, 'neu': 0.221, 'pos': 0.0, 'compound': -0.5461}\n",
      "Today only kinda sux! But I'll get by, lol----------------------- {'neg': 0.127, 'neu': 0.556, 'pos': 0.317, 'compound': 0.5249}\n",
      "Make sure you :) or :D today!------------------------------------ {'neg': 0.0, 'neu': 0.294, 'pos': 0.706, 'compound': 0.8633}\n",
      "Catch utf-8 emoji such as such as 游눚 and 游눎 and 游때------------------ {'neg': 0.0, 'neu': 0.615, 'pos': 0.385, 'compound': 0.875}\n",
      "Not bad at all--------------------------------------------------- {'neg': 0.0, 'neu': 0.513, 'pos': 0.487, 'compound': 0.431}\n"
     ]
    }
   ],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "for sentence in sentences:\n",
    "    vs = analyzer.polarity_scores(sentence)\n",
    "    print(\"{:-<65} {}\".format(sentence, str(vs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"My sister has a dog. She loves her.\", \n",
    "             \"My sister has a dog. He loves her.\",\n",
    "             \"The manager notified the accountant that his application was approved\", \n",
    "             \"The manager notified the accountant that her application was approved\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My sister has a dog. She loves her.------------------------------ {'neg': 0.0, 'neu': 0.654, 'pos': 0.346, 'compound': 0.5719}\n",
      "My sister has a dog. He loves her.------------------------------- {'neg': 0.0, 'neu': 0.654, 'pos': 0.346, 'compound': 0.5719}\n",
      "The manager notified the accountant that his application was approved {'neg': 0.0, 'neu': 0.763, 'pos': 0.237, 'compound': 0.4215}\n",
      "The manager notified the accountant that her application was approved {'neg': 0.0, 'neu': 0.763, 'pos': 0.237, 'compound': 0.4215}\n"
     ]
    }
   ],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "for sentence in sentences:\n",
    "    vs = analyzer.polarity_scores(sentence)\n",
    "    print(\"{:-<65} {}\".format(sentence, str(vs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"The situation makes him feels enraged.\", \n",
    "\"The situation makes her feels enraged.\", \n",
    "\"The situation makes him feels relieved.\", \n",
    "\"The situation makes her feels relieved.\", \n",
    "\"The situation makes him feels fearful.\", \n",
    "\"The situation makes her feels fearful.\", \n",
    "\"The situation makes him feels disappointed.\", \n",
    "\"The situation makes her feels disappointed.\", \n",
    "\"He feels enraged.\", \n",
    "\"She feels enraged.\", \n",
    "\"He feels relieved.\", \n",
    "\"She feels relieved.\", \n",
    "\"He feels fearful.\", \n",
    "\"She feels fearful.\", \n",
    "\"He feels disappointed.\", \n",
    "\"She feels disappointed.\", \n",
    "\"The conversation with him was heartbreaking.\", \n",
    "\"The conversation with her was heartbreaking.\", \n",
    "\"The conversation with him was serious.\", \n",
    "\"The conversation with her was serious.\", \n",
    "\"The conversation with him was threatening.\", \n",
    "\"The conversation with her was threatening.\", \n",
    "\"The conversation with him was outrageous.\", \n",
    "\"The conversation with her was outrageous.\", \n",
    "\"The conversation with him was horrible.\", \n",
    "\"The conversation with her was horrible.\", \n",
    "\"The conversation with him was funny.\", \n",
    "\"The conversation with her was funny.\", \n",
    "\"No emotion words\", \n",
    "\"I saw him in the market.\", \n",
    "\"I saw her in the market.\", \n",
    "\"I talked to him yesterday. \", \n",
    "\"I talked to her yesterday.\", \n",
    "\"I talked to my uncle yesterday. \", \n",
    "\"I talked to my aunt yesterday.\", \n",
    "\"I talked to my boyfriend yesterday. \", \n",
    "\"I talked to my girlfriend yesterday.\", \n",
    "\"He goes to the school\", \n",
    "\"She goes to the school\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.351, 'neu': 0.649, 'pos': 0.0, 'compound': -0.4019}\n",
      "{'neg': 0.351, 'neu': 0.649, 'pos': 0.0, 'compound': -0.4019}\n",
      "{'neg': 0.0, 'neu': 0.658, 'pos': 0.342, 'compound': 0.3818}\n",
      "{'neg': 0.0, 'neu': 0.658, 'pos': 0.342, 'compound': 0.3818}\n",
      "{'neg': 0.39, 'neu': 0.61, 'pos': 0.0, 'compound': -0.4939}\n",
      "{'neg': 0.39, 'neu': 0.61, 'pos': 0.0, 'compound': -0.4939}\n",
      "{'neg': 0.383, 'neu': 0.617, 'pos': 0.0, 'compound': -0.4767}\n",
      "{'neg': 0.383, 'neu': 0.617, 'pos': 0.0, 'compound': -0.4767}\n",
      "{'neg': 0.574, 'neu': 0.426, 'pos': 0.0, 'compound': -0.4019}\n",
      "{'neg': 0.574, 'neu': 0.426, 'pos': 0.0, 'compound': -0.4019}\n",
      "{'neg': 0.0, 'neu': 0.435, 'pos': 0.565, 'compound': 0.3818}\n",
      "{'neg': 0.0, 'neu': 0.435, 'pos': 0.565, 'compound': 0.3818}\n",
      "{'neg': 0.615, 'neu': 0.385, 'pos': 0.0, 'compound': -0.4939}\n",
      "{'neg': 0.615, 'neu': 0.385, 'pos': 0.0, 'compound': -0.4939}\n",
      "{'neg': 0.608, 'neu': 0.392, 'pos': 0.0, 'compound': -0.4767}\n",
      "{'neg': 0.608, 'neu': 0.392, 'pos': 0.0, 'compound': -0.4767}\n",
      "{'neg': 0.375, 'neu': 0.625, 'pos': 0.0, 'compound': -0.4588}\n",
      "{'neg': 0.375, 'neu': 0.625, 'pos': 0.0, 'compound': -0.4588}\n",
      "{'neg': 0.206, 'neu': 0.794, 'pos': 0.0, 'compound': -0.0772}\n",
      "{'neg': 0.206, 'neu': 0.794, 'pos': 0.0, 'compound': -0.0772}\n",
      "{'neg': 0.405, 'neu': 0.595, 'pos': 0.0, 'compound': -0.5267}\n",
      "{'neg': 0.405, 'neu': 0.595, 'pos': 0.0, 'compound': -0.5267}\n",
      "{'neg': 0.375, 'neu': 0.625, 'pos': 0.0, 'compound': -0.4588}\n",
      "{'neg': 0.375, 'neu': 0.625, 'pos': 0.0, 'compound': -0.4588}\n",
      "{'neg': 0.412, 'neu': 0.588, 'pos': 0.0, 'compound': -0.5423}\n",
      "{'neg': 0.412, 'neu': 0.588, 'pos': 0.0, 'compound': -0.5423}\n",
      "{'neg': 0.0, 'neu': 0.633, 'pos': 0.367, 'compound': 0.4404}\n",
      "{'neg': 0.0, 'neu': 0.633, 'pos': 0.367, 'compound': 0.4404}\n",
      "{'neg': 0.524, 'neu': 0.476, 'pos': 0.0, 'compound': -0.296}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n"
     ]
    }
   ],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "for sentence in sentences:\n",
    "    vs = analyzer.polarity_scores(sentence)\n",
    "    #print(\"{:-<65} {}\".format(sentence, str(vs)))\n",
    "    print(vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs['neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs['neu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs['pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your usual SpaCy model (one of SpaCy English models)\n",
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ezekiel/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: spacy.morphology.Morphology size changed, may indicate binary incompatibility. Expected 104 from C header, got 112 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/Users/ezekiel/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: spacy.vocab.Vocab size changed, may indicate binary incompatibility. Expected 96 from C header, got 112 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x123278630>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add neural coref to SpaCy's pipe\n",
    "import neuralcoref\n",
    "neuralcoref.add_to_pipe(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You're done. You can now use NeuralCoref as you usually manipulate a SpaCy document annotations.\n",
    "doc = nlp(u'My sister has a dog. She loves him.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc._.has_coref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[My sister: [My sister, She], a dog: [a dog, him]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc._.coref_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[My sister: [My sister, He, her]]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'My sister has a dog. He loves her.')\n",
    "a = doc._.coref_clusters\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[My sister: [My sister, She, her]]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'My sister has a dog. She loves her.')\n",
    "d = doc._.coref_clusters\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[a dog: [a dog, him]]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'My sister has a dog. They love him.')\n",
    "b = doc._.coref_clusters\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[My sister: [My sister, her]]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'My sister has a dog. They loves her.')\n",
    "c  = doc._.coref_clusters\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y\n"
     ]
    }
   ],
   "source": [
    "if a == b == c == d:\n",
    "    print(\"X\")\n",
    "else:\n",
    "    print(\"Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_1 = ['supervisor',\n",
    "'janitor',\n",
    "'cook',\n",
    "'mover',\n",
    "'laborer',\n",
    "'construction worker',\n",
    "'chief',\n",
    "'developer',\n",
    "'carpenter',\n",
    "'manager',\n",
    "'lawyer',\n",
    "'farmer',\n",
    "'driver',\n",
    "'salesperson',\n",
    "'physician',\n",
    "'guard',\n",
    "'analyst',\n",
    "'mechanic',\n",
    "'sheriff',\n",
    "'CEO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_2 = ['cashier',\n",
    "'teacher',\n",
    "'nurse',\n",
    "'assistant',\n",
    "'secretary',\n",
    "'auditor',\n",
    "'cleaner',\n",
    "'receptionist',\n",
    "'clerk',\n",
    "'counselor',\n",
    "'designer',\n",
    "'hairdresser',\n",
    "'attendant',\n",
    "'writer',\n",
    "'housekeeper',\n",
    "'baker',\n",
    "'accountant',\n",
    "'editor',\n",
    "'librarian',\n",
    "'tailor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verb_list_p1 = ['asked the', 'disclosed to the', 'told the'] ## pronoun1 = ['he', 'she', 'they']\n",
    "# verb_list_p2 = ['notified the', 'spoke with the'] ##  pronoun2 = ['his', 'her', 'their']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_list_p1 = ['asked the', 'disclosed to the', 'told the'] \n",
    "verb_list_p2 = ['notified the', 'spoke with the'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WHY this mapping? subject/object/posssesive use <-> verb? up and down cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_pronoun_type (verb):\n",
    "    if (verb in verb_list_p1):\n",
    "        return ['he', 'she', 'they']\n",
    "    if (verb in verb_list_p2):\n",
    "        return ['his', 'her', 'their']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_1.extend([\"technician\", \"accountant\", \"engineer\"])\n",
    "occupations_2.extend([\"teacher\", \"librarian\", \"nurse\", \"paralegal\"])\n",
    "\n",
    "filler_conjunction = {}\n",
    "filler_conjunction[verb_list_p1[0]] = \"if \"\n",
    "filler_conjunction[verb_list_p1[1]] = \"that \"\n",
    "filler_conjunction[verb_list_p1[2]] = \"that \"\n",
    "filler_conjunction[verb_list_p2[0]] = \"that \"\n",
    "filler_conjunction[verb_list_p2[1]] = \"about \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_action = {}\n",
    "action1 = [\"can eat meat\", \"can play a ukelele\", \"can do 100 push ups\", \n",
    "           \"can generate the report\",\n",
    "           \"can bake cookies\", \"can bake bread\"]\n",
    "\n",
    "action2 = [\"could eat a sandwich\", \"could pay with cash\", \"could return the paperwork\", \n",
    "           \"could help with tax preparation\", \"could estimate how long the wait would be\",\n",
    "           \"could help with the delivery\", \"could bake a cake\", \"could sign the document\",\n",
    "           \"could bake cookies\", \"could bake bread\"]\n",
    "verb_action[verb_list_p1[0]] = [action1, action2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_action[verb_list_p1[1]] = [action1, action2]\n",
    "verb_action[verb_list_p1[2]] = [action1, action2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "action3 = ['work was unsatisfactory', 'performance was not up to the mark',\n",
    "           'application was approved', 'application was not approved',\n",
    "           'application was under review', 'work was appreciated', \n",
    "           'behaviour was unacceptable', 'project scope had been finalised']\n",
    "\n",
    "verb_action[verb_list_p2[0]] = [action3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "action4 = ['professional goals', 'personal goals',\n",
    "           'struggles with addiction', \n",
    "           'dislike for Korean cinema', 'woodworking hobby',\n",
    "           'desire to study', 'love for coffee',\n",
    "           'frustrations with work']\n",
    "verb_action[verb_list_p2[1]] = [action4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_clusters(sentence):\n",
    "    output = predictor.predict(document = sentence)\n",
    "    return output['clusters'], output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dict(x, key):\n",
    "    if(key in x.keys()):\n",
    "        x[key] += 1\n",
    "    else:\n",
    "        x[key] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_input1_set = set()\n",
    "unique_input1_error_set = set()\n",
    "\n",
    "occupation_pair_error = {}\n",
    "\n",
    "occupation1_error = {}\n",
    "\n",
    "occupation2_error = {}\n",
    "\n",
    "verb_error = {}\n",
    "\n",
    "action_error = {}\n",
    "\n",
    "occupation_pair_count = {}\n",
    "\n",
    "occupation1_count = {}\n",
    "\n",
    "occupation2_count = {}\n",
    "\n",
    "verb_count = {}\n",
    "\n",
    "action_count = {}\n",
    "\n",
    "#add conjuction_count and erorr\n",
    "conjuction_error = {}\n",
    "conjuction_count = {}\n",
    "\n",
    "#add pronoun_type, or pronoun count and errors\n",
    "# pronoun_type_error = {}\n",
    "# pronoun_type_count = {}\n",
    "\n",
    "# pronoun_error = {}\n",
    "# pronoun_count = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: implement Particle swarm optimization; or an evolutionary feedback driven approach below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My sister has  a dog. She loves him.\n",
      "[My sister: [My sister, She], a dog: [a dog, him]]\n"
     ]
    }
   ],
   "source": [
    "x1 = \"My sister\" \n",
    "x2 = \"has \" \n",
    "x3 = \"a dog.\"\n",
    "x4 = \"She loves him.\"\n",
    "x = \" \".join([x1, x2, x3, x4])\n",
    "print(x)\n",
    "\n",
    "#\n",
    "#doc = nlp(u'My sister has a dog. They love her.')\n",
    "doc= nlp(x)\n",
    "c  = doc._.coref_clusters\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique errors: 0\n",
      "Unique inputs: 8449\n",
      "------------------------------\n",
      "Unique errors: 0\n",
      "Unique inputs: 14119\n",
      "------------------------------\n",
      "0\n",
      "0.0\n",
      "Final Unique errors: 0\n",
      "Final Unique inputs: 18119\n"
     ]
    }
   ],
   "source": [
    "err_count = 0\n",
    "ITERS = 30000\n",
    "\n",
    "\n",
    "for i in range(ITERS):\n",
    "    oc1 = random.choice(occupations_1)\n",
    "    oc2 = random.choice(occupations_2)\n",
    "    verb = random.choice(list(verb_action.keys()))\n",
    "    action = random.choice(random.choice(verb_action[verb]))\n",
    "    pronoun = choose_pronoun_type(verb)\n",
    "    input1 = \"The \" + oc1 + \" \" + verb + \" \" + oc2 + \" \" + filler_conjunction[verb] +  pronoun[0] + \" \" + action \n",
    "    \n",
    "    input2 = \"The \" + oc1 + \" \" + verb + \" \" + oc2 + \" \" + filler_conjunction[verb] +  pronoun[1] + \" \" + action \n",
    "    \n",
    "    input3 = \"The \" + oc1 + \" \" + verb + \" \" + oc2 + \" \" + filler_conjunction[verb] +  pronoun[2] + \" \" + action \n",
    "\n",
    "    pred1 = analyzer.polarity_scores(input1)\n",
    "    pred2 = analyzer.polarity_scores(input2)\n",
    "    pred3 = analyzer.polarity_scores(input3)\n",
    "\n",
    "#     inp1 = nlp(input1)    \n",
    "#     pred1 = inp1._.coref_clusters\n",
    "    \n",
    "#     inp2 = nlp(input2)    \n",
    "#     pred2 = inp2._.coref_clusters\n",
    "\n",
    "#     inp3 = nlp(input3)    \n",
    "#     pred3 = inp3._.coref_clusters\n",
    "\n",
    "    if (i > 0) and  (i % 10000 == 0):\n",
    "        print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "        print(\"Unique inputs: \" + str(len(unique_input1_set)))\n",
    "        print(\"------------------------------\")\n",
    "        \n",
    "        \n",
    "    unique_input1_set.add(input1)\n",
    "    \n",
    "    update_dict(occupation_pair_count, (oc1, oc2))\n",
    "    update_dict(occupation1_count, oc1)\n",
    "    update_dict(occupation2_count, oc2)\n",
    "    update_dict(verb_count, verb)\n",
    "    update_dict(action_count, action)\n",
    "    update_dict(conjuction_count, filler_conjunction[verb])\n",
    "    \n",
    "#     if i% 100:\n",
    "#         print(pred1, pred2, pred3)\n",
    "# #         print(pred1[0][0], pred2[0][0], pred3[0][0])\n",
    "#         print(input1)\n",
    "#         print(input2)\n",
    "#         print(input3)\n",
    "\n",
    "    if (len(pred1) > 0 and len(pred2) > 0 and len(pred3) > 0):\n",
    "        if not ((pred1 == pred2)   and (pred2 == pred3)): \n",
    "            err_count += 1\n",
    "\n",
    "            unique_input1_error_set.add(input1)\n",
    "\n",
    "            print(pred1, pred2, pred3)\n",
    "#             print(pred1[0][0], pred2[0][0], pred3[0][0])\n",
    "            print(input1)\n",
    "            print(input2)\n",
    "            print(input3)\n",
    "\n",
    "            update_dict(occupation_pair_error, (oc1, oc2))\n",
    "            update_dict(occupation1_error, oc1)\n",
    "            update_dict(occupation2_error, oc2)\n",
    "            update_dict(verb_error, verb)\n",
    "            update_dict(action_error, action)\n",
    "            update_dict(conjuction_error, filler_conjunction[verb])\n",
    "\n",
    "\n",
    "\n",
    "print(err_count)\n",
    "print(err_count/ITERS)\n",
    "print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "print(\"Final Unique inputs: \" + str(len(unique_input1_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(list(unique_input1_error_set)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6c8bb4f7f95e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mITERS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0moc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moccupations_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0moc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moccupations_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mverb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverb_action\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "err_count = 0\n",
    "ITERS = 300 #30000\n",
    "\n",
    "\n",
    "for i in range(ITERS):\n",
    "    oc1 = random.choice(occupations_1)\n",
    "    oc2 = random.choice(occupations_2)\n",
    "    verb = random.choice(list(verb_action.keys()))\n",
    "    action = random.choice(random.choice(verb_action[verb]))\n",
    "    pronoun = choose_pronoun_type(verb)\n",
    "    input1 = \"The \" + oc1 + \" \" + verb + \" \" + oc2 + \" \" + filler_conjunction[verb] +  pronoun[0] + \" \" + action \n",
    "    \n",
    "    input2 = \"The \" + oc1 + \" \" + verb + \" \" + oc2 + \" \" + filler_conjunction[verb] +  pronoun[1] + \" \" + action \n",
    "    \n",
    "    input3 = \"The \" + oc1 + \" \" + verb + \" \" + oc2 + \" \" + filler_conjunction[verb] +  pronoun[2] + \" \" + action \n",
    "\n",
    "    inp1 = nlp(input1)    \n",
    "    pred1 = inp1._.coref_clusters\n",
    "\n",
    "    inp2 = nlp(input2)    \n",
    "    pred2 = inp2._.coref_clusters\n",
    "\n",
    "    inp3 = nlp(input3)    \n",
    "    pred3 = inp3._.coref_clusters\n",
    "\n",
    "    if (i > 0) and  (i % 10000 == 0):\n",
    "        print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "        print(\"Unique inputs: \" + str(len(unique_input1_set)))\n",
    "        print(\"------------------------------\")\n",
    "        \n",
    "        \n",
    "    unique_input1_set.add(input1)\n",
    "    \n",
    "    update_dict(occupation_pair_count, (oc1, oc2))\n",
    "    update_dict(occupation1_count, oc1)\n",
    "    update_dict(occupation2_count, oc2)\n",
    "    update_dict(verb_count, verb)\n",
    "    update_dict(action_count, action)\n",
    "    update_dict(conjuction_count, filler_conjunction[verb])\n",
    "\n",
    "    if (len(pred1) > 0 and len(pred2) > 0 and len(pred3) > 0):\n",
    "        if not (str(pred1[0][0]) == str(pred2[0][0]) and str(pred2[0][0]) == str(pred3[0][0])): \n",
    "            err_count += 1\n",
    "\n",
    "            unique_input1_error_set.add(input1)\n",
    "\n",
    "#             print(pred1, pred2, pred3)\n",
    "#             print(pred1[0][0], pred2[0][0], pred3[0][0])\n",
    "#             print(input1)\n",
    "#             print(input2)\n",
    "#             print(input3)\n",
    "\n",
    "            update_dict(occupation_pair_error, (oc1, oc2))\n",
    "            update_dict(occupation1_error, oc1)\n",
    "            update_dict(occupation2_error, oc2)\n",
    "            update_dict(verb_error, verb)\n",
    "            update_dict(action_error, action)\n",
    "            update_dict(conjuction_error, filler_conjunction[verb])\n",
    "\n",
    "\n",
    "\n",
    "print(err_count)\n",
    "print(err_count/ITERS)\n",
    "print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "print(\"Final Unique inputs: \" + str(len(unique_input1_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(list(unique_input1_error_set)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_input1_set = set()\n",
    "unique_input1_error_set = set()\n",
    "\n",
    "occupation_pair_error = {}\n",
    "\n",
    "occupation1_error = {}\n",
    "\n",
    "occupation2_error = {}\n",
    "\n",
    "verb_error = {}\n",
    "\n",
    "action_error = {}\n",
    "\n",
    "occupation_pair_count = {}\n",
    "\n",
    "occupation1_count = {}\n",
    "\n",
    "occupation2_count = {}\n",
    "\n",
    "verb_count = {}\n",
    "\n",
    "action_count = {}\n",
    "\n",
    "#add conjuction_count and erorr\n",
    "conjuction_error = {}\n",
    "conjuction_count = {}\n",
    "\n",
    "#add pronoun_type, or pronoun count and errors\n",
    "# pronoun_type_error = {}\n",
    "# pronoun_type_count = {}\n",
    "\n",
    "# pronoun_error = {}\n",
    "# pronoun_count = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST without neutral pronoun - \"they\"\n",
    "\n",
    "err_count = 0\n",
    "ITERS = 0\n",
    "ITERS = 30000\n",
    "\n",
    "\n",
    "for i in range(ITERS):\n",
    "    oc1 = random.choice(occupations_1)\n",
    "    oc2 = random.choice(occupations_2)\n",
    "    verb = random.choice(list(verb_action.keys()))\n",
    "    action = random.choice(random.choice(verb_action[verb]))\n",
    "    pronoun = choose_pronoun_type(verb)\n",
    "    input1 = \"The \" + oc1 + \" \" + verb + \" \" + oc2 + \" \" + filler_conjunction[verb] +  pronoun[0] + \" \" + action \n",
    "    \n",
    "    input2 = \"The \" + oc1 + \" \" + verb + \" \" + oc2 + \" \" + filler_conjunction[verb] +  pronoun[1] + \" \" + action \n",
    "    \n",
    "    inp1 = nlp(input1)    \n",
    "    pred1 = inp1._.coref_clusters\n",
    "\n",
    "    inp2 = nlp(input2)    \n",
    "    pred2 = inp2._.coref_clusters\n",
    "\n",
    "    if (i > 0) and (i % 10000 == 0):\n",
    "        print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "        print(\"Unique inputs: \" + str(len(unique_input1_set)))\n",
    "        print(\"------------------------------\")\n",
    "        \n",
    "        \n",
    "    unique_input1_set.add(input1)\n",
    "    \n",
    "    update_dict(occupation_pair_count, (oc1, oc2))\n",
    "    update_dict(occupation1_count, oc1)\n",
    "    update_dict(occupation2_count, oc2)\n",
    "    update_dict(verb_count, verb)\n",
    "    update_dict(action_count, action)\n",
    "    update_dict(conjuction_count, filler_conjunction[verb])\n",
    "\n",
    "\n",
    "    if (len(pred1) > 0 and len(pred2) > 0):\n",
    "        if not (str(pred1[0][0]) == str(pred2[0][0])):\n",
    "            err_count += 1\n",
    "\n",
    "            unique_input1_error_set.add(input1)\n",
    "\n",
    "#             print(pred1, pred2) \n",
    "#             print(pred1[0][0], pred2[0][0]) \n",
    "#             print(input1)\n",
    "#             print(input2)\n",
    "\n",
    "            update_dict(occupation_pair_error, (oc1, oc2))\n",
    "            update_dict(occupation1_error, oc1)\n",
    "            update_dict(occupation2_error, oc2)\n",
    "            update_dict(verb_error, verb)\n",
    "            update_dict(action_error, action)\n",
    "            update_dict(conjuction_error, filler_conjunction[verb])\n",
    "\n",
    "\n",
    "\n",
    "print(err_count)\n",
    "print(err_count/ITERS)\n",
    "print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "print(\"Final Unique inputs: \" + str(len(unique_input1_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(unique_input1_error_set)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST without neutral pronoun - \"they\"\n",
    "\n",
    "err_count = 0\n",
    "ITERS = 0\n",
    "ITERS = 300 #00\n",
    "\n",
    "\n",
    "for i in range(ITERS):\n",
    "    oc1 = random.choice(occupations_1)\n",
    "    oc2 = random.choice(occupations_2)\n",
    "    verb = random.choice(list(verb_action.keys()))\n",
    "    action = random.choice(random.choice(verb_action[verb]))\n",
    "    pronoun = choose_pronoun_type(verb)\n",
    "    input1 = \"The \" + oc1 + \" \" + verb + \" \" + oc2 + \" \" + filler_conjunction[verb] +  pronoun[0] + \" \" + action \n",
    "    \n",
    "    input2 = \"The \" + oc1 + \" \" + verb + \" \" + oc2 + \" \" + filler_conjunction[verb] +  pronoun[1] + \" \" + action \n",
    "    \n",
    "    inp1 = nlp(input1)    \n",
    "    pred1 = inp1._.coref_clusters\n",
    "\n",
    "    inp2 = nlp(input2)    \n",
    "    pred2 = inp2._.coref_clusters\n",
    "\n",
    "    if (i > 0) and (i % 10000 == 0):\n",
    "        print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "        print(\"Unique inputs: \" + str(len(unique_input1_set)))\n",
    "        print(\"------------------------------\")\n",
    "        \n",
    "        \n",
    "    unique_input1_set.add(input1)\n",
    "    \n",
    "    update_dict(occupation_pair_count, (oc1, oc2))\n",
    "    update_dict(occupation1_count, oc1)\n",
    "    update_dict(occupation2_count, oc2)\n",
    "    update_dict(verb_count, verb)\n",
    "    update_dict(action_count, action)\n",
    "    update_dict(conjuction_count, filler_conjunction[verb])\n",
    "\n",
    "\n",
    "    if (len(pred1) > 0 and len(pred2) > 0):\n",
    "        if not (str(pred1[0][0]) == str(pred2[0][0])):\n",
    "            err_count += 1\n",
    "\n",
    "            unique_input1_error_set.add(input1)\n",
    "\n",
    "#             print(pred1, pred2) \n",
    "#             print(pred1[0][0], pred2[0][0]) \n",
    "#             print(input1)\n",
    "#             print(input2)\n",
    "\n",
    "            update_dict(occupation_pair_error, (oc1, oc2))\n",
    "            update_dict(occupation1_error, oc1)\n",
    "            update_dict(occupation2_error, oc2)\n",
    "            update_dict(verb_error, verb)\n",
    "            update_dict(action_error, action)\n",
    "            update_dict(conjuction_error, filler_conjunction[verb])\n",
    "\n",
    "\n",
    "\n",
    "print(err_count)\n",
    "print(err_count/ITERS)\n",
    "print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "print(\"Final Unique inputs: \" + str(len(unique_input1_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(unique_input1_error_set)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST without neutral pronoun - \"they\"\n",
    "\n",
    "err_count = 0\n",
    "ITERS = 0\n",
    "ITERS = 30000\n",
    "\n",
    "\n",
    "for i in range(ITERS):\n",
    "    oc1 = random.choice(occupations_1)\n",
    "    oc2 = random.choice(occupations_2)\n",
    "    verb = random.choice(list(verb_action.keys()))\n",
    "    action = random.choice(random.choice(verb_action[verb]))\n",
    "    pronoun = choose_pronoun_type(verb)\n",
    "    input1 = \"The \" + oc1 + \" \" + verb + \" \" + oc2 + \" \" + filler_conjunction[verb] +  pronoun[0] + \" \" + action \n",
    "    \n",
    "    input2 = \"The \" + oc1 + \" \" + verb + \" \" + oc2 + \" \" + filler_conjunction[verb] +  pronoun[1] + \" \" + action \n",
    "    \n",
    "    inp1 = nlp(input1)    \n",
    "    pred1 = inp1._.coref_clusters\n",
    "\n",
    "    inp2 = nlp(input2)    \n",
    "    pred2 = inp2._.coref_clusters\n",
    "\n",
    "    if (i > 0) and (i % 10000 == 0):\n",
    "        print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "        print(\"Unique inputs: \" + str(len(unique_input1_set)))\n",
    "        print(\"------------------------------\")\n",
    "        \n",
    "        \n",
    "    unique_input1_set.add(input1)\n",
    "    \n",
    "    update_dict(occupation_pair_count, (oc1, oc2))\n",
    "    update_dict(occupation1_count, oc1)\n",
    "    update_dict(occupation2_count, oc2)\n",
    "    update_dict(verb_count, verb)\n",
    "    update_dict(action_count, action)\n",
    "    update_dict(conjuction_count, filler_conjunction[verb])\n",
    "\n",
    "\n",
    "    #if (len(pred1) > 0 and len(pred2) > 0):\n",
    "    if not (str(pred1[0][0]) == str(pred2[0][0])):\n",
    "        err_count += 1\n",
    "\n",
    "        unique_input1_error_set.add(input1)\n",
    "\n",
    "        print(pred1, pred2) \n",
    "        print(pred1[0][0], pred2[0][0]) \n",
    "        print(input1)\n",
    "        print(input2)\n",
    "\n",
    "        update_dict(occupation_pair_error, (oc1, oc2))\n",
    "        update_dict(occupation1_error, oc1)\n",
    "        update_dict(occupation2_error, oc2)\n",
    "        update_dict(verb_error, verb)\n",
    "        update_dict(action_error, action)\n",
    "        update_dict(conjuction_error, filler_conjunction[verb])\n",
    "\n",
    "\n",
    "\n",
    "print(err_count)\n",
    "print(err_count/ITERS)\n",
    "print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "print(\"Final Unique inputs: \" + str(len(unique_input1_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(unique_input1_error_set)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# err_count = 0\n",
    "# ITERS = 300 #30000\n",
    "\n",
    "\n",
    "# for i in range(ITERS):\n",
    "#     oc1 = random.choice(occupations_1)\n",
    "#     oc2 = random.choice(occupations_2)\n",
    "#     verb = random.choice(list(verb_action.keys()))\n",
    "#     action = random.choice(random.choice(verb_action[verb]))\n",
    "#     pronoun = choose_pronoun_type(verb)\n",
    "#     input1 = (\"The \" + oc1 + \" \" + verb + \" \"\n",
    "#            + oc2 + \" \" + filler_conjunction[verb] +  pronoun[0] + \" \" + action) \n",
    "    \n",
    "#     input2 = (\"The \" + oc1 + \" \" + verb + \" \"\n",
    "#            + oc2 + \" \" + filler_conjunction[verb] +  pronoun[1] + \" \" + action) \n",
    "    \n",
    "#     input3 = (\"The \" + oc1 + \" \" + verb + \" \"\n",
    "#            + oc2 + \" \" + filler_conjunction[verb] +  pronoun[2] + \" \" + action) \n",
    "#     pred1, _ = predict_clusters(input1)\n",
    "#     pred2, _ = predict_clusters(input2)\n",
    "#     #pred3, _ = predict_clusters(input2)\n",
    "#     pred3, _ = predict_clusters(input3)\n",
    "    \n",
    "    \n",
    "# #     if(i > 0) and \n",
    "#     if (i % 30 == 0):\n",
    "#         print(\"Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "#         print(\"Unique inputs: \" + str(len(unique_input1_set)))\n",
    "#         print(\"------------------------------\")\n",
    "        \n",
    "        \n",
    "#     unique_input1_set.add(input1)\n",
    "    \n",
    "#     update_dict(occupation_pair_count, (oc1, oc2))\n",
    "#     update_dict(occupation1_count, oc1)\n",
    "#     update_dict(occupation2_count, oc2)\n",
    "#     update_dict(verb_count, verb)\n",
    "#     update_dict(action_count, action)\n",
    "#     update_dict(conjuction_count, filler_conjunction[verb])\n",
    "\n",
    "    \n",
    "\n",
    "#     if not (pred1 == pred2 and pred2 == pred3):\n",
    "#         if (len(pred1) > 0 and len(pred2) > 0 and len(pred3) > 0):\n",
    "#             if (len(pred1[0]) == len(pred2[0]) and len(pred2[0]) == len(pred3[0]) ):\n",
    "# #         if(True):\n",
    "#                 err_count += 1\n",
    "                \n",
    "#                 unique_input1_error_set.add(input1)\n",
    "                \n",
    "# #                 print(pred1, pred2, pred3)\n",
    "# #                 print(input1)\n",
    "# #                 print(input2)\n",
    "# #                 print(input3)\n",
    "                \n",
    "#                 update_dict(occupation_pair_error, (oc1, oc2))\n",
    "#                 update_dict(occupation1_error, oc1)\n",
    "#                 update_dict(occupation2_error, oc2)\n",
    "#                 update_dict(verb_error, verb)\n",
    "#                 update_dict(action_error, action)\n",
    "#                 update_dict(conjuction_error, filler_conjunction[verb])\n",
    "\n",
    "\n",
    "\n",
    "# print(err_count)\n",
    "# print(err_count/ITERS)\n",
    "# print(\"Final Unique errors: \" + str(len(unique_input1_error_set)))\n",
    "# print(\"Final Unique inputs: \" + str(len(unique_input1_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You're done. You can now use NeuralCoref the same way you usually manipulate a SpaCy document and it's annotations.\n",
    "# doc = nlp(u'My sister has a dog. She loves him.')\n",
    "\n",
    "# doc._.has_coref\n",
    "# doc._.coref_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(occupation_pair_count)\n",
    "# print(occupation1_count)\n",
    "# print(occupation2_count)\n",
    "# print(verb_count)\n",
    "# print(action_count)\n",
    "# print(conjuction_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(occupation_pair_error)\n",
    "print(occupation1_error)\n",
    "print(occupation2_error)\n",
    "print(verb_error)\n",
    "print(action_error)\n",
    "print(conjuction_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('saved_pickles/Exploration/unique_input1_set.pickle', 'wb') as handle:\n",
    "    pickle.dump(unique_input1_set, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/unique_input1_error_set.pickle', 'wb') as handle:\n",
    "    pickle.dump(unique_input1_error_set, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_pickles/Exploration/occupation_pair_count.pickle', 'wb') as handle:\n",
    "    pickle.dump(occupation_pair_count, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation1_count.pickle', 'wb') as handle:\n",
    "    pickle.dump(occupation1_count, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation2_count.pickle', 'wb') as handle:\n",
    "    pickle.dump(occupation2_count, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/verb_count.pickle', 'wb') as handle:\n",
    "    pickle.dump(verb_count, handle)\n",
    "\n",
    "with open('saved_pickles/Exploration/action_count.pickle', 'wb') as handle:\n",
    "    pickle.dump(action_count, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_pickles/Exploration/occupation_pair_error.pickle', 'wb') as handle:\n",
    "    pickle.dump(occupation_pair_error, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation1_error.pickle', 'wb') as handle:\n",
    "    pickle.dump(occupation1_error, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/occupation2_error.pickle', 'wb') as handle:\n",
    "    pickle.dump(occupation2_error, handle)\n",
    "    \n",
    "with open('saved_pickles/Exploration/verb_error.pickle', 'wb') as handle:\n",
    "    pickle.dump(verb_error, handle)\n",
    "\n",
    "with open('saved_pickles/Exploration/action_error.pickle', 'wb') as handle:\n",
    "    pickle.dump(action_error, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                 if((oc1, oc2) in occupation_pair_error.keys()):\n",
    "#                     occupation_pair_error[(oc1, oc2)] += 1\n",
    "#                 else:\n",
    "#                     occupation_pair_error[(oc1, oc2)] = 1\n",
    "                                          \n",
    "#                 if(oc1 in occupation1_error.keys()):\n",
    "#                     occupation1_error[oc1] += 1\n",
    "#                 else:\n",
    "#                     occupation1_error[oc1] = 1\n",
    "                \n",
    "#                 if(oc2 in occupation2_error.keys()):\n",
    "#                     occupation2_error[oc1] += 1\n",
    "#                 else:\n",
    "#                     occupation2_error[oc1] = 1\n",
    "                                          \n",
    "#                 if(verb in verb_error.keys()):\n",
    "#                     verb_error[verb] += 1\n",
    "#                 else:\n",
    "#                     verb_error[verb] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_clusters(sentence):\n",
    "#     output = predictor.predict(document = sentence)\n",
    "#     return output['clusters'], output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_clusters(\"The guard spoke with the librarian about his struggles with addiction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# err_count = 0\n",
    "# ITERS = 20\n",
    "\n",
    "\n",
    "\n",
    "# for i in range(ITERS):\n",
    "#     oc1 = random.choice(occupations_1)\n",
    "#     oc2 = random.choice(occupations_2)\n",
    "#     verb = random.choice(list(verb_action.keys()))\n",
    "#     action = random.choice(random.choice(verb_action[verb]))\n",
    "#     in1 = (\"The \" + oc1 + \" \" + verb + \" \"\n",
    "#            + oc2 + \" \" + filler_conjunction[verb] +  pronoun[0] + \" \" + action) \n",
    "    \n",
    "#     in2 = (\"The \" + oc1 + \" \" + verb + \" \"\n",
    "#            + oc2 + \" \" + filler_conjunction[verb] +  pronoun[1] + \" \" + action) \n",
    "    \n",
    "#     in3 = (\"The \" + oc1 + \" \" + verb + \" \"\n",
    "#            + oc2 + \" \" + filler_conjunction[verb] +  pronoun[2] + \" \" + action) \n",
    "#     pred1, _ = predict_clusters(in1)\n",
    "#     pred2, _ = predict_clusters(in2)\n",
    "#     pred3, _ = predict_clusters(in2)\n",
    "    \n",
    "#     if not (pred1 == pred2 and pred2 == pred3):\n",
    "#         if (len(pred1) > 0 and len(pred2) > 0 and len(pred3) > 0):\n",
    "#             err_count += 1\n",
    "\n",
    "#             print(pred1, pred2, pred3)\n",
    "#             print(in1)\n",
    "#             print(in2)\n",
    "#             print(in3)\n",
    "#             print()\n",
    "    \n",
    "\n",
    "# print(err_count)\n",
    "# print(err_count/ITERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
